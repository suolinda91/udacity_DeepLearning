{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "In this notebook, you'll build a GAN using convolutional layers in the generator and discriminator. This is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored in 2016 and has seen impressive results in generating new images; you can read the [original paper, here](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "You'll be training DCGAN on the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. These are color images of different classes, such as airplanes, dogs or trucks. This dataset is much more complex and diverse than the MNIST dataset and justifies the use of the DCGAN architecture.\n",
    "\n",
    "<img src='assets/cifar10_data.png' width=80% />\n",
    "\n",
    "\n",
    "So, our goal is to create a DCGAN that can generate new, realistic-looking images. We'll go through the following steps to do this:\n",
    "* **Load in and pre-process the CIFAR10 dataset**\n",
    "* Define discriminator and generator networks\n",
    "* **Train these adversarial networks**\n",
    "* **Visualize the loss over time and some sample, generated images**\n",
    "\n",
    "In this notebook, we will focus on defining the networks.\n",
    "\n",
    "#### Deeper Convolutional Networks\n",
    "\n",
    "Since this dataset is more complex than our MNIST data, we'll need a deeper network to accurately identify patterns in these images and be able to generate new ones. Specifically, we'll use a series of convolutional or transpose convolutional layers in the discriminator and generator. It's also necessary to use batch normalization to get these convolutional networks to train. \n",
    "\n",
    "Besides these changes in network structure, training the discriminator and generator networks should be the same as before. That is, the discriminator will alternate training on real and fake (generated) images, and the generator will aim to trick the discriminator into thinking that its generated images are real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this cell once to install the dependency. \n",
    "# You will have to restart the kernel once the package is installed.\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Here you can download the CIFAR10 dataset. It's a dataset built-in to the PyTorch datasets library. We can load in training data, transform it into Tensor datatypes, then create dataloaders to batch our data into a desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Tensor transform\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# CIFAR training datasets\n",
    "cifar_train = datasets.CIFAR10(root='data/', train=True, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "# build DataLoaders for CIFAR10 dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=cifar_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "Here I'm showing a small sample of the images. Each of these is 32x32 with 3 color channels (RGB). These are the real, training images that we'll pass to the discriminator. Notice that each image has _one_ associated, numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: scaling from -1 to 1\n",
    "\n",
    "We need to do a bit of pre-processing; we know that the output of our `tanh` activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current range\n",
    "img = images[0]\n",
    "\n",
    "print('Min: ', img.min())\n",
    "print('Max: ', img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    # assume x is scaled to (0, 1)\n",
    "    # scale to feature_range and return scaled x\n",
    "    ####\n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled range\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print('Scaled min: ', scaled_img.min())\n",
    "print('Scaled max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the Model\n",
    "\n",
    "A GAN is comprised of two adversarial networks, a discriminator and a generator. Let's use the models we created in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "Here you'll build the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. \n",
    "* The inputs to the discriminator are 32x32x3 tensor images\n",
    "* You'll want a few convolutional, hidden layers\n",
    "* Then a fully connected layer for the output; as before, we want a sigmoid output, but we'll add that in the loss function, [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss), later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block is made of 3 layers: Conv -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - batch_norm: whether to use batch norm or not\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, batch_norm: bool = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    The discriminator model adapted from the DCGAN paper. It should only contains a few layers.\n",
    "    args:\n",
    "    - conv_dim: control the number of filters\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_dim: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        # 32x32 input\n",
    "        self.conv1 = ConvBlock(3, conv_dim, 4, batch_norm=False) # first layer, no batch_norm\n",
    "        # 16x16 out\n",
    "        self.conv2 = ConvBlock(conv_dim, conv_dim*2, 4)\n",
    "        # 8x8 out\n",
    "        self.conv3 = ConvBlock(conv_dim*2, conv_dim*4, 4)\n",
    "        # 4x4 out\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        # final, fully-connected layer\n",
    "        self.fc = nn.Linear(conv_dim*4*4*4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # all hidden layers + leaky relu activation\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten\n",
    "        x = self.flatten(x)\n",
    "        # final output layer\n",
    "        x = self.fc(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Next, you'll build the generator network. The input will be our noise vector `z`, as before. And, the output will be a $tanh$ output, but this time with size 32x32 which is the size of our CIFAR10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A \"de-convolutional\" block is made of 3 layers: ConvTranspose -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - stride: stride of the conv layer\n",
    "    - padding: padding of the conv layer\n",
    "    - batch_norm: whether to use batch norm or not\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int,\n",
    "                 padding: int,\n",
    "                 batch_norm: bool = True):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.deconv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator model adapted from DCGAN\n",
    "    args:\n",
    "    - latent_dim: dimension of the latent vector\n",
    "    - conv_dim: control the number of filters in the convtranspose layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, conv_dim: int = 32):\n",
    "        super(Generator, self).__init__()\n",
    "        # transpose conv layers\n",
    "        self.deconv1 = DeconvBlock(latent_dim, conv_dim*4, 4, 1, 0)\n",
    "        self.deconv2 = DeconvBlock(conv_dim*4, conv_dim*2, 4, 2, 1)\n",
    "        self.deconv3 = DeconvBlock(conv_dim*2, conv_dim, 4, 2, 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(conv_dim, 3, 4, stride=2, padding=1)\n",
    "        self.last_activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.last_activation(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build complete network\n",
    "\n",
    "Define your models' hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you've passed in the correct input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "conv_dim = 32\n",
    "z_size = 100\n",
    "\n",
    "# define discriminator and generator\n",
    "D = Discriminator(conv_dim)\n",
    "G = Generator(latent_dim=z_size, conv_dim=conv_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GPU\n",
    "\n",
    "Check if you can train on GPU. If you can, set this as a variable and move your models to GPU. \n",
    "> Later, we'll also move any inputs our models and loss functions see (real_images, z, and ground truth labels) to GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    # move models to GPU\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses. And this will be exactly the same as before.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
    "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "The losses will by binary cross entropy loss with logits, which we can get with [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). This combines a `sigmoid` activation function **and** and binary cross entropy loss in one function.\n",
    "\n",
    "For the real images, we want `D(real_images) = 1`. That is, we want the discriminator to classify the real images with a label = 1, indicating that these are real. The discriminator loss for the fake data is similar. We want `D(fake_images) = 0`, where the fake images are the _generator output_, `fake_images = G(z)`. \n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "The generator loss will look similar only with flipped labels. The generator's goal is to get `D(fake_images) = 1`. In this case, the labels are **flipped** to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Not much new here, but notice how I am using a small learning rate and custom parameters for the Adam optimizers, This is based on some research into DCGAN model convergence.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "GANs are very sensitive to hyperparameters. A lot of experimentation goes into finding the best hyperparameters such that the generator and discriminator don't overpower each other. Try out your own hyperparameters or read [the DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf) to see what worked for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# IMPLEMENT BELOW\n",
    "# params\n",
    "lr = None\n",
    "beta1 = None\n",
    "beta2 = None\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = None\n",
    "g_optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. We'll use our functions `real_loss` and `fake_loss` to help us calculate the discriminator losses in all of the following cases.\n",
    "\n",
    "### Discriminator training\n",
    "1. Compute the discriminator loss on real, training images        \n",
    "2. Generate fake images\n",
    "3. Compute the discriminator loss on fake, generated images     \n",
    "4. Add up real and fake loss\n",
    "5. Perform backpropagation + an optimization step to update the discriminator's weights\n",
    "\n",
    "### Generator training\n",
    "1. Generate fake images\n",
    "2. Compute the discriminator loss on fake images, using **flipped** labels!\n",
    "3. Perform backpropagation + an optimization step to update the generator's weights\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "As we train, we'll also print out some loss statistics and save some generated \"fake\" samples.\n",
    "\n",
    "**Evaluation mode**\n",
    "\n",
    "Notice that, when we call our generator to create the samples to display, we set our model to evaluation mode: `G.eval()`. That's so the batch normalization layers will use the population statistics rather than the batch statistics (as they do during training), *and* so dropout layers will operate in eval() mode; not turning off any nodes for generating samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(14,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img +1)*255 / (2)).astype(np.uint8) # rescale to pixel range (0-255)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "num_epochs = 50\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size, 1, 1))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # important rescaling step\n",
    "        real_images = scale(real_images)\n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATOR\n",
    "        # ============================================\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATOR\n",
    "        # =========================================\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            losses.append((d_loss.item(), g_loss.item()))\n",
    "            # print discriminator and generator loss\n",
    "            time = str(datetime.now()).split('.')[0]\n",
    "            print(f'{time} | Epoch [{epoch+1}/{num_epochs}] | Batch {batch_i}/{len(train_loader)} | d_loss: {d_loss.item():.4f} | g_loss: {g_loss.item():.4f}')\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    samples_z = G(fixed_z)\n",
    "    samples.append(samples_z)\n",
    "    view_samples(-1, samples)\n",
    "    G.train() # back to training mode\n",
    "\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "Here we'll plot the training losses for the generator and discriminator, recorded after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
