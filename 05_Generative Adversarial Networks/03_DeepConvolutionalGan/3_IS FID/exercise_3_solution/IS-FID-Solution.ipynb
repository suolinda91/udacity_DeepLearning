{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Score (IS) and Frechet Inception Distance (FID)\n",
    "\n",
    "Both the Inception Score (or IS) and the Frechet Inception Distance (or FID) are metrics to assess the quality of images generated by a GAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated data\n",
    "\n",
    "To calculate the Inception Score and the Frechet Inception Distance, we are going to use some generated samples from the previous exercise. The `samples.pickle` contains 800 samples recorded over 50 epochs (16 samples per epoch). We can visualize them below. The samples are saved as `np.array` and contain images in the [0, 255] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell once to install the dependency\n",
    "# you will have to restart the kernel afterwards\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for visualization.\n",
    "\n",
    "def view_samples(epoch: int, samples: List[np.array]):\n",
    "    fig, axes = plt.subplots(figsize=(14,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[16 * epoch: 16 * (epoch + 1)]):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../samples.pkl', 'rb') as f:\n",
    "    samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_samples(49, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception score\n",
    "\n",
    "The Inception Score was introduced by the [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf) paper. This metric relies on the following approach:\n",
    "* generated images are fed through the [Inception model](https://arxiv.org/pdf/1409.4842.pdf) pretrained on the ImageNet dataset\n",
    "* the probability distribution for each image should have [low-entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)). In other words, the model should output high probabilities for a single class and be confident that the image contains one object. We call this distribution the **conditional label distribution**.\n",
    "* the probability distribution of all the classes over the whole dataset should have high entropy, meaning that the generative model creates images with high variability. We call this distribution the **marginal distribution**.\n",
    "\n",
    "The inception score is calculated using the [Kullbackâ€“Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL Divergence). \n",
    "\n",
    "<img src='../assets/kl_divergence.png' width=50% />\n",
    "\n",
    "\n",
    "The KL divergence is a measure of how two distributions are similar. Since we want the conditional label distribution to have [low entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) (think of a very low spread gaussian for example) and the marginal distribution to have high entropy (think uniform distribution), we want to **maximize the KL divergence**. \n",
    "\n",
    "<img src='../assets/fid_score.png' width=90% />\n",
    "\n",
    "[This article](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a) explains the inception score in great depth. \n",
    "\n",
    "#### Tips:\n",
    "* look at the [scipy.stats.entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html) function for the kl divergence calculation.\n",
    "* you can find the pytorch Inception_v3 model [here](https://pytorch.org/hub/pytorch_vision_inception_v3/). Don't forget to reshape the inputs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models.inception import inception_v3\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(samples: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    This function calculates the score for each sample.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception_model.eval()\n",
    "    inception_model.cuda()\n",
    "    \n",
    "    # preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])    \n",
    "    \n",
    "    # forward pass\n",
    "    scores = []\n",
    "    for image in samples:\n",
    "        image = Image.fromarray(image)\n",
    "        input_tensor = preprocess(image)\n",
    "        input_batch = input_tensor.unsqueeze(0).cuda()\n",
    "        output = inception_model(input_batch)\n",
    "        probs = torch.nn.functional.softmax(output[0].cpu().detach(), dim=0)\n",
    "        scores.append(probs.numpy())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = calculate_scores(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inception_score(scores: List[np.ndarray]) -> float:\n",
    "    total_scores = []\n",
    "    py = np.mean(scores, axis=0)\n",
    "\n",
    "    # calculate the kl divergence\n",
    "    kl = []\n",
    "    for pyx in scores:\n",
    "        ent = entropy(pyx, py)\n",
    "        kl.append(ent)\n",
    "    return np.exp(np.mean(kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_score = get_inception_score(scores)\n",
    "print(inception_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frechet Inception Distance\n",
    "\n",
    "The Frechet Inception Distance was first introduced by the [GANs Trained by a Two Time-Scale Update Rule\n",
    "Converge to a Local Nash Equilibrium](https://arxiv.org/pdf/1706.08500.pdf) paper. \n",
    "\n",
    "The Inception Score was a groundbreaking metric for measure GAN performances but it has a major flaw: **it does not take into account the statistics of the \"real\" dataset the GAN is supposed to mimic and only uses the generated images.**\n",
    "\n",
    "\n",
    "The FID takes a different approach:\n",
    "* for each image in the generated dataset **and** the target (real) dataset, get the latent representation by running the Inception model. The latent representation of each image is the output of the penultimate layer, before the final classification layer.\n",
    "* calculate the mean and the covariance of the real distribution ($m_{r}$ and $C_{r}$)\n",
    "* calculate the mean and the covariance of the generated distribution ($m_{g}$ and $C_{g}$)\n",
    "* calculate the Frechet Distance defined below:\n",
    "\n",
    "$FID = ||m_{r} - m_{g}||^{2}_{2} + Tr(C_{r}) + Tr(C_{g}) - 2 Tr(C_{r}C_{g})^{1/2}$\n",
    "\n",
    "where $||.||_{2}$ is the L-2 norm and $Tr$ the trace of the covariance matrices. \n",
    "\n",
    "\n",
    "In this exercise, we will simply ask you to implement the FID calculation, assuming that you already have calculated the mean and covariance of both distribution.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"black\">\n",
    "<b>Click for tips</b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "* you can calculate the trace of the covariance matrices using `np.trace`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fid(mu1: np.array, sigma1: np.array, mu2: np.array, sigma2: np.array):\n",
    "    \"\"\"\n",
    "    Calculate the FID. \n",
    "    \"\"\"\n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # calculate the square root of the dot product of the covariance matrices\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # calculate the trace\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1)\n",
    "            + np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = np.random.randn(100, 2048)\n",
    "mu1 = np.mean(samples1, axis=0)\n",
    "sigma1 = np.cov(samples1)\n",
    "\n",
    "samples2 = np.random.randn(100, 2048)\n",
    "mu2 = np.mean(samples2, axis=0)\n",
    "sigma2 = np.cov(samples2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = get_fid(mu1, sigma1, mu2, sigma2)\n",
    "print(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
