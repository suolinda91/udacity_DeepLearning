{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN, Image-to-Image Translation\n",
    "\n",
    "In this notebook, we're going to define and train a CycleGAN to read in an image from a set $X$ and transform it so that it looks as if it belongs in set $Y$. Specifically, we'll look at a set of images of [Yosemite national park](https://en.wikipedia.org/wiki/Yosemite_National_Park) taken either during the summer of winter. The seasons are our two domains!\n",
    "\n",
    ">The objective will be to train generators that learn to transform an image from domain $X$ into an image that looks like it came from domain $Y$ (and vice versa). \n",
    "\n",
    "Some examples of image data in both sets are pictured below.\n",
    "\n",
    "<img src='../assets/XY_season_images.png' width=80% />\n",
    "\n",
    "### Unpaired Training Data\n",
    "\n",
    "These images do not come with labels, but CycleGANs give us a way to learn the mapping between one image domain and another using an **unsupervised** approach. A CycleGAN is designed for image-to-image translation and it learns from unpaired training data. This means that in order to train a generator to translate images from domain $X$ to domain $Y$, we do not have to have exact correspondences between individual images in those domains. For example, in [the paper that introduced CycleGANs](https://arxiv.org/abs/1703.10593), the authors are able to translate between images of horses and zebras, even though there are no images of a zebra in exactly the same position as a horse or with exactly the same background, etc. Thus, CycleGANs enable learning a mapping from one domain $X$ to another domain $Y$ without having to find perfectly-matched, training pairs!\n",
    "\n",
    "<img src='../assets/horse2zebra.jpg' width=60% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN and Exercises Structure\n",
    "\n",
    "A CycleGAN is made of two types of networks: **discriminators, and generators**. In this example, the discriminators are responsible for classifying images as real or fake (for both $X$ and $Y$ kinds of images). The generators are responsible for generating convincing, fake images for both kinds of images. \n",
    "\n",
    "To successfully train a cycle gan, we need to do the following: \n",
    "\n",
    ">1. You'll load in the image data using PyTorch's DataLoader class to efficiently read in images from a specified directory.\n",
    "2. **Then, you'll be tasked with defining the CycleGAN architecture according to provided specifications. You'll define the generator models and implement the different loss functions.**\n",
    "3. You'll complete the training cycle by calculating the adversarial and cycle consistency losses for the generator and discriminator network and completing a number of training epochs. *It's suggested that you enable GPU usage for training.* \n",
    "4. Finally, you'll evaluate your model by looking at the loss over time and looking at sample, generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Model\n",
    "\n",
    "A CycleGAN is made of two discriminator and two generator networks.\n",
    "\n",
    "## Discriminators\n",
    "\n",
    "The discriminators, $D_X$ and $D_Y$, in this CycleGAN are convolutional neural networks that see an image and attempt to classify it as real or fake. In this case, real is indicated by an output close to 1 and fake as close to 0. The discriminators have the following architecture:\n",
    "\n",
    "<img src='../assets/discriminator_layers.png' width=80% />\n",
    "\n",
    "This network sees a 128x128x3 image, and passes it through 5 convolutional layers that downsample the image by a factor of 2. The first four convolutional layers have a BatchNorm and ReLu activation function applied to their output, and the last acts as a classification layer that outputs one value.\n",
    "\n",
    "**The discriminator architecture is not very different from the DCGAN architecture and therefore we will focus on implementing the generator only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "The generators, `G_XtoY` and `G_YtoX` (sometimes called F), are made of an **encoder**, a conv net that is responsible for turning an image into a smaller feature representation, and a **decoder**, a *transpose_conv* net that is responsible for turning that representation into an transformed image. These generators, one from XtoY and one from YtoX, have the following architecture:\n",
    "\n",
    "<img src='../assets/cyclegan_generator_ex.png' width=90% />\n",
    "\n",
    "This network sees a 128x128x3 image, compresses it into a feature representation as it goes through three convolutional layers and reaches a series of residual blocks. It goes through a few (typically 6 or more) of these residual blocks, then it goes through three transpose convolutional layers (sometimes called *de-conv* layers) which upsample the output of the resnet blocks and create a new image!\n",
    "\n",
    "Note that most of the convolutional and transpose-convolutional layers have BatchNorm and ReLu functions applied to their outputs with the exception of the final transpose convolutional layer, which has a `tanh` activation function applied to the output. Also, the residual blocks are made of convolutional and batch normalization layers, which we'll go over in more detail, next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Residual Block Class\n",
    "\n",
    "To define the generators, you're expected to define a `ResidualBlock` class which will help you connect the encoder and decoder portions of the generators. You might be wondering, what exactly is a Resnet block? It may sound familiar from something like ResNet50 for image classification, pictured below.\n",
    "\n",
    "<img src='../assets/resnet_50.png' width=90%/>\n",
    "\n",
    "ResNet blocks rely on connecting the output of one layer with the input of an earlier layer. The motivation for this structure is as follows: very deep neural networks can be difficult to train. Deeper networks are more likely to have vanishing or exploding gradients and, therefore, have trouble reaching convergence; batch normalization helps with this a bit. However, during training, we often see that deep networks respond with a kind of training degradation. Essentially, the training accuracy stops improving and gets saturated at some point during training. In the worst cases, deep models would see their training accuracy actually worsen over time!\n",
    "\n",
    "One solution to this problem is to use **Resnet blocks** that allow us to learn so-called *residual functions* as they are applied to layer inputs. You can read more about this proposed architecture in the paper, [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) by Kaiming He et. al, and the below image is from that paper.\n",
    "\n",
    "<img src='../assets/resnet_block.png' width=80%/>\n",
    "\n",
    "### Residual Functions\n",
    "\n",
    "Usually, when we create a deep learning model, the model (several layers with activations applied) is responsible for learning a mapping, `M`, from an input `x` to an output `y`.\n",
    ">`M(x) = y` (Equation 1)\n",
    "\n",
    "Instead of learning a direct mapping from `x` to `y`, we can instead define a **residual function**\n",
    "> `F(x) = M(x) - x`\n",
    "\n",
    "This looks at the difference between a mapping applied to x and the original input, x. `F(x)` is, typically, two convolutional layers + normalization layer and a ReLu in between. These convolutional layers should have the same number of inputs as outputs. This mapping can then be written as the following; a function of the residual function and the input x. The addition step creates a kind of loop that connects the input x to the output, y:\n",
    ">`M(x) = F(x) + x` (Equation 2) or\n",
    "\n",
    ">`y = F(x) + x` (Equation 3)\n",
    "\n",
    "#### Optimizing a Residual Function\n",
    "\n",
    "The idea is that it is easier to optimize this residual function `F(x)` than it is to optimize the original mapping `M(x)`. Consider an example; what if we want `y = x`?\n",
    "\n",
    "From our first, direct mapping equation, **Equation 1**, we could set `M(x) = x` but it is easier to solve the residual equation `F(x) = 0`, which, when plugged in to **Equation 3**, yields `y = x`.\n",
    "\n",
    "\n",
    "### Defining the `ResidualBlock` Class\n",
    "\n",
    "To define the `ResidualBlock` class, we'll define residual functions (a series of layers), apply them to an input x and add them to that same input. This is defined just like any other neural network, with an `__init__` function and the addition step in the `forward` function. \n",
    "\n",
    "In our case, you'll want to define the residual block as:\n",
    "* Two convolutional layers with the same size input and output\n",
    "* Batch normalization applied to the outputs of the convolutional layers\n",
    "* A ReLu function on the output of the *first* convolutional layer\n",
    "\n",
    "Then, in the `forward` function, add the input x to this residual block. Feel free to use the helper `conv` function from above to create this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block is made of 3 layers: Conv -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - stride: stride of the conv layer\n",
    "    - activation: whether to use an activation function or not\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 activation: bool = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Defines a residual block.\n",
    "       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\n",
    "       These blocks allow a model to learn an effective transformation from one domain to another.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # conv_dim = number of inputs\n",
    "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n",
    "        # layers should have the same shape input as output; I suggest a kernel_size of 3\n",
    "        self.conv_block1 = ConvBlock(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3)\n",
    "        self.conv_block2 = ConvBlock(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, activation=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply a ReLu activation the outputs of the first layer\n",
    "        # return a summed output, x + resnet_block(x)\n",
    "        out_1 = self.conv_block1(x)\n",
    "        out_2 = x + self.conv_block2(out_1)\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose Convolutional Helper Function\n",
    "\n",
    "To define the generators, you're expected to use the above `conv` function, `ResidualBlock` class, and the below `deconv` helper function, which creates a transpose convolutional layer + an optional batchnorm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A \"de-convolutional\" block is made of 3 layers: ConvTranspose -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - stride: stride of the conv layer\n",
    "    - padding: padding of the conv layer\n",
    "    - batch_norm: whether to use batch norm or not\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int = 2,\n",
    "                 padding: int = 1,\n",
    "                 batch_norm: bool = True):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.deconv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Generator Architecture\n",
    "\n",
    "* Complete the `__init__` function with the specified 3 layer **encoder** convolutional net, a series of residual blocks (the number of which is given by `n_res_blocks`), and then a 3 layer **decoder** transpose convolutional net.\n",
    "* Then complete the `forward` function to define the forward behavior of the generators. Recall that the last layer has a `tanh` activation function.\n",
    "\n",
    "Both $G_{XtoY}$ and $G_{YtoX}$ have the same architecture, so we only need to define one class, and later instantiate two generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim: int = 64, n_res_blocks: int = 6):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "\n",
    "        # 1. Define the encoder part of the generator\n",
    "        \n",
    "        # initial convolutional layer given, below\n",
    "        self.conv1 = ConvBlock(3, conv_dim, 4, stride=2)\n",
    "        self.conv2 = ConvBlock(conv_dim, conv_dim*2, 4, stride=2)\n",
    "        self.conv3 = ConvBlock(conv_dim*2, conv_dim*4, 4, stride=2)\n",
    "\n",
    "        # 2. Define the resnet part of the generator\n",
    "        # Residual blocks\n",
    "        self.res_layers = nn.ModuleList()\n",
    "        for layer in range(n_res_blocks):\n",
    "            self.res_layers.append(ResidualBlock(conv_dim*4))\n",
    "\n",
    "        # 3. Define the decoder part of the generator\n",
    "        # two transpose convolutional layers and a third that looks a lot like the initial conv layer\n",
    "        self.deconv1 = DeconvBlock(conv_dim*4, conv_dim*2, 4)\n",
    "        self.deconv2 = DeconvBlock(conv_dim*2, conv_dim, 4)\n",
    "        # no batch norm on last layer\n",
    "        self.deconv3 = nn.ConvTranspose2d(conv_dim, 3, 4, 2, 1, bias=False)\n",
    "        \n",
    "        self.final_act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Given an image x, returns a transformed image.\"\"\"\n",
    "        # define feedforward behavior, applying activations as necessary\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        for layer in self.res_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.deconv1(out)\n",
    "        out = self.deconv2(out)\n",
    "        # tanh applied to last layer\n",
    "        out = self.deconv3(out)\n",
    "        out = self.final_act(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = CycleGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cycle_generator(generator: nn.Module):\n",
    "    image = torch.randn(1, 3, 128, 128)\n",
    "    output = generator(image)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_cycle_generator(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses\n",
    "\n",
    "Computing the discriminator and the generator losses are key to getting a CycleGAN to train.\n",
    "\n",
    "<img src='../assets/CycleGAN_loss.png' width=90% height=90% />\n",
    "\n",
    "**Image from [original paper](https://arxiv.org/abs/1703.10593) by Jun-Yan Zhu et. al.**\n",
    "\n",
    "* The CycleGAN contains two mapping functions $G: X \\rightarrow Y$ and $F: Y \\rightarrow X$, and associated adversarial discriminators $D_Y$ and $D_X$. **(a)** $D_Y$ encourages $G$ to translate $X$ into outputs indistinguishable from domain $Y$, and vice versa for $D_X$ and $F$.\n",
    "\n",
    "* To further regularize the mappings, we introduce two cycle consistency losses that capture the intuition that if\n",
    "we translate from one domain to the other and back again we should arrive at where we started. **(b)** Forward cycle-consistency loss and **(c)** backward cycle-consistency loss.\n",
    "\n",
    "## Least Squares GANs\n",
    "\n",
    "We've seen that regular GANs treat the discriminator as a classifier with the sigmoid cross entropy loss function. However, this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we'll use a least squares loss function for the discriminator. This structure is also referred to as a least squares GAN or LSGAN, and you can [read the original paper on LSGANs, here](https://arxiv.org/pdf/1611.04076.pdf). The authors show that LSGANs are able to generate higher quality images than regular GANs and that this loss type is a bit more stable during training! \n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "The discriminator losses will be mean squared errors between the output of the discriminator, given an image, and the target value, 0 or 1, depending on whether it should classify that image as fake or real. For example, for a *real* image, `x`, we can train $D_X$ by looking at how close it is to recognizing and image `x` as real using the mean squared error:\n",
    "\n",
    "```\n",
    "out_x = D_X(x)\n",
    "real_err = torch.mean((out_x-1)**2)\n",
    "```\n",
    "\n",
    "### Generator Losses\n",
    "\n",
    "Calculating the generator losses will look somewhat similar to calculating the discriminator loss; there will still be steps in which you generate fake images that look like they belong to the set of $X$ images but are based on real images in set $Y$, and vice versa. You'll compute the \"real loss\" on those generated images by looking at the output of the discriminator as it's applied to these _fake_ images; this time, your generator aims to make the discriminator classify these fake images as *real* images. \n",
    "\n",
    "#### Cycle Consistency Loss\n",
    "\n",
    "In addition to the adversarial losses, the generator loss terms will also include the **cycle consistency loss**. This loss is a measure of how good a reconstructed image is, when compared to an original image. \n",
    "\n",
    "Say you have a fake, generated image, `x_hat`, and a real image, `y`. You can get a reconstructed `y_hat` by applying `G_XtoY(x_hat) = y_hat` and then check to see if this reconstruction `y_hat` and the orginal image `y` match. For this, we recommed calculating the L1 loss, which is an absolute difference, between reconstructed and real images. You may also choose to multiply this loss by some weight value `lambda_weight` to convey its importance.\n",
    "\n",
    "<img src='../assets/reconstruction_error.png' width=80% height=80% />\n",
    "\n",
    "The total generator loss will be the sum of the generator losses and the forward and backward cycle consistency losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Define Loss Functions\n",
    "\n",
    "To help us calculate the discriminator and gnerator losses during training, let's define some helpful loss functions. Here, we'll define three.\n",
    "1. `real_mse_loss` that looks at the output of a discriminator and returns the error based on how close that output is to being classified as real. This should be a mean squared error.\n",
    "2. `fake_mse_loss` that looks at the output of a discriminator and returns the error based on how close that output is to being classified as fake. This should be a mean squared error.\n",
    "3. `cycle_consistency_loss` that looks at a set of real image and a set of reconstructed/generated images, and returns the mean absolute error between them. This has a `lambda_weight` parameter that will weight the mean absolute error in a batch.\n",
    "\n",
    "It's recommended that you take a [look at the original, CycleGAN paper](https://arxiv.org/pdf/1703.10593.pdf) to get a starting value for `lambda_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_mse_loss(D_out: torch.Tensor) -> torch.Tensor:\n",
    "    # how close is the produced output from being \"real\"?\n",
    "    return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(D_out: torch.Tensor) -> torch.Tensor:\n",
    "    # how close is the produced output from being \"false\"?\n",
    "    return torch.mean(D_out**2)\n",
    "\n",
    "def cycle_consistency_loss(real_im: torch.Tensor, \n",
    "                           reconstructed_im: torch.Tensor, \n",
    "                           lambda_weight: torch.Tensor) -> torch.Tensor:\n",
    "    # calculate reconstruction loss \n",
    "    # as absolute value difference between the real and reconstructed images\n",
    "    reconstr_loss = torch.mean(torch.abs(real_im - reconstructed_im))\n",
    "    # return weighted loss\n",
    "    return lambda_weight * reconstr_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_losses(real_mse_loss, fake_mse_loss, cycle_consistency_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
