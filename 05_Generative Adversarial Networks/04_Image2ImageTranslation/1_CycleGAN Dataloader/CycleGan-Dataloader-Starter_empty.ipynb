{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN, Image-to-Image Translation\n",
    "\n",
    "In this notebook, we're going to define and train a CycleGAN to read in an image from a set $X$ and transform it so that it looks as if it belongs in set $Y$. Specifically, we'll look at a set of images of [Yosemite national park](https://en.wikipedia.org/wiki/Yosemite_National_Park) taken either during the summer of winter. The seasons are our two domains!\n",
    "\n",
    ">The objective will be to train generators that learn to transform an image from domain $X$ into an image that looks like it came from domain $Y$ (and vice versa). \n",
    "\n",
    "Some examples of image data in both sets are pictured below.\n",
    "\n",
    "<img src='assets/XY_season_images.png' width=80% />\n",
    "\n",
    "### Unpaired Training Data\n",
    "\n",
    "These images do not come with labels, but CycleGANs give us a way to learn the mapping between one image domain and another using an **unsupervised** approach. A CycleGAN is designed for image-to-image translation and it learns from unpaired training data. This means that in order to train a generator to translate images from domain $X$ to domain $Y$, we do not have to have exact correspondences between individual images in those domains. For example, in [the paper that introduced CycleGANs](https://arxiv.org/abs/1703.10593), the authors are able to translate between images of horses and zebras, even though there are no images of a zebra in exactly the same position as a horse or with exactly the same background, etc. Thus, CycleGANs enable learning a mapping from one domain $X$ to another domain $Y$ without having to find perfectly-matched, training pairs!\n",
    "\n",
    "<img src='assets/horse2zebra.jpg' width=50% />\n",
    "\n",
    "### CycleGAN and Exercises Structure\n",
    "\n",
    "A CycleGAN is made of two types of networks: **discriminators, and generators**. In this example, the discriminators are responsible for classifying images as real or fake (for both $X$ and $Y$ kinds of images). The generators are responsible for generating convincing, fake images for both kinds of images. \n",
    "\n",
    "To successfully train a cycle gan, we need to do the following: \n",
    "\n",
    ">1. **You'll load in the image data using PyTorch's DataLoader class to efficiently read in images from a specified directory.** \n",
    "2. Then, you'll be tasked with defining the CycleGAN architecture according to provided specifications. You'll define the discriminator and the generator models. \n",
    "3. You'll complete the training cycle by calculating the adversarial and cycle consistency losses for the generator and discriminator network and completing a number of training epochs. *It's suggested that you enable GPU usage for training.* \n",
    "4. Finally, you'll evaluate your model by looking at the loss over time and looking at sample, generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "The `get_data_loader` function returns training and test DataLoaders that can load data efficiently and in specified batches. The function has the following parameters:\n",
    "* `image_type`: `summer` or `winter`,  the names of the directories where the X and Y images are stored\n",
    "* `image_dir`: name of the main image directory, which holds all training and test images\n",
    "* `image_size`: resized, square image dimension (all images will be resized to this dim)\n",
    "* `batch_size`: number of images in one batch of data\n",
    "\n",
    "The test data is strictly for feeding to our generators, later on, so we can visualize some generated samples on fixed, test data.\n",
    "\n",
    "You can see that this function is also responsible for making sure our images are of the right, square size (128x128x3) and converted into Tensor image types.\n",
    "\n",
    "**It's suggested that you use the default values of these parameters.**\n",
    "\n",
    "Note: If you are trying this code on a different set of data, you may get better results with larger `image_size` and `batch_size` parameters. If you change the `batch_size`, make sure that you create complete batches in the training loop otherwise you may get an error when trying to save sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and transforming data\n",
    "from typing import Callable\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize \n",
    "\n",
    "# visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(image_type: str, \n",
    "                    transform: Callable,\n",
    "                    image_dir: str = 'summer2winter_yosemite', \n",
    "                    batch_size: int = 16, \n",
    "                    num_workers: int = 0):\n",
    "    \"\"\"Returns training and test data loaders for a given image type, either 'summer' or 'winter'. \n",
    "       These images will be resized to 128x128x3, by default, converted into Tensors, and normalized.\n",
    "    \"\"\"\n",
    "    #### \n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataloaders for images from the two domains X and Y\n",
    "# image_type = directory names for our data\n",
    "\n",
    "# resize and normalize the images\n",
    "image_size = 128\n",
    "transform = Compose([Resize(image_size), # resize to 128x128\n",
    "                     ToTensor()])\n",
    "    \n",
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='summer', \n",
    "                                                  transform=transform,\n",
    "                                                  image_dir='summer2winter_yosemite/',)\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='winter', \n",
    "                                                  transform=transform,\n",
    "                                                  image_dir='summer2winter_yosemite/',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some Training Images\n",
    "\n",
    "Below we provide a function `imshow` that reshape some given images and converts them to NumPy images so that they can be displayed by `plt`. This cell should display a grid that contains a batch of image data from set $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper imshow function\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "\n",
    "# get some images from X\n",
    "dataiter = iter(dataloader_X)\n",
    "# the \"_\" is a placeholder for no labels\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some images from Y\n",
    "dataiter = iter(dataloader_Y)\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: scaling from -1 to 1\n",
    "\n",
    "We need to do a bit of pre-processing; we know that the output of our `tanh` activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.) \n",
    "\n",
    "This time, try to use the [`Normalize` transform](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html) to output values directly between -1 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='summer', \n",
    "                                                  transform=transform,\n",
    "                                                  image_dir='summer2winter_yosemite/',)\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='winter', \n",
    "                                                  transform=transform,\n",
    "                                                  image_dir='summer2winter_yosemite/',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in dataloader_X:\n",
    "    print(f'Image min: {image.min()}, Image max: {image.max()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in dataloader_Y:\n",
    "    print(f'Image min: {image.min()}, Image max: {image.max()}')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
