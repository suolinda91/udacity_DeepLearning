{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein Loss and Gradient Penalty\n",
    "\n",
    "In the course, we discussed the limitations of the binary cross entropy loss (BCE Loss). It can lead to:\n",
    "* Mode collapse: when the generator is stuck in a single mode of the distribution it is trying to replicate. For example, a generator training on the MNIST dataset would be stuck in generating images of certain digits only, as shown below.\n",
    "\n",
    "<img src='assets/collapse.png' width=50% />\n",
    "\n",
    "* Vanishing gradient: it's a common problem with many neural network architectures, but is very common when training GANs. Because the discriminator's task is much easier than the generator's, the discriminator tends to converge faster and reach a high accuracy. The discriminator loss gets close to zero and the gradients become very small, leading to that vanishing gradient problem. \n",
    "\n",
    "In this notebook, you will:\n",
    "* implement the Wasserstein Loss\n",
    "* implement two types of gradient penalties\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein Loss\n",
    "\n",
    "The Wasserstein GAN paper introduced a new type of loss function: the Wasserstein Distance. \n",
    "\n",
    "We are now reshaping the problem GANs are solving: instead of having a loss function that classifies a distribution as being real or not, we have a loss function that tries to minimize the distance between the real and the fake distribution. The difference is subtle but plays a big role in the stability of GANs.\n",
    "\n",
    "<img src='assets/gradient_replace.png' width=90% />\n",
    "\n",
    "The discriminator is now called a **critic** because its job is not really to distinguish between real and fake anymore but to maximize the distance between the two distributions. However, we will be using both terms interchangeably for the sake of clarity. \n",
    "\n",
    "The Wasserstein loss can be calculated using the formula below:\n",
    "\n",
    "<center>$\\min_{g} \\max_{c} E(c(x)) - E(c(g(z)))$</center>\n",
    "\n",
    "You are now familiar with the minimax function. The main difference with the BCE Loss is the disapperance of the logs!\n",
    "\n",
    "### First exercise: Implement the Wasserstein Loss\n",
    "\n",
    "The Wasserstein Loss (W-Loss) is taking the vector of logits outputed by the discriminator as input. In comparison, the BCE Loss was taking the probabilities (logits after a softmax layer) as inputs. The discriminator W-Loss is trying to maximize the mean value of the logits of real images and minize the mean value of the logits of fake images. The generator W-Loss is trying to maximize the mean value of the logits of fake images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_w_loss(real_logits: torch.Tensor, fake_logits: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Wasserstein Discriminator Loss\n",
    "    \n",
    "    args:\n",
    "    - real_logits: vector of logits outputed by the discriminator with a real input image\n",
    "    - fake_logits: vector of logits outputed by the discriminator with a fake input image \n",
    "    \"\"\"\n",
    "    ####\n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_g_loss(fake_logits: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Wasserstein Generator Loss\n",
    "    \n",
    "    args:\n",
    "    - fake_logits: vector of logits outputed by the discriminator with a fake input image \n",
    "    \"\"\"\n",
    "    ####\n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests.check_disc_w_loss(disc_w_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_gen_w_loss(disc_g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient penalty\n",
    "\n",
    "To train a GAN with the Wasserstein Loss, the discriminator (or critic) must be 1-Lipschitz continuous. \n",
    "\n",
    "The 1-Lipschitz continuity constraint implies that the norm of the gradient of the function must be below 1. In other words, for a function $f(x)$:\n",
    "\n",
    "</br>\n",
    "<center>$|| \\frac{df}{dx} || < 1$</center>\n",
    "\n",
    "Because the W-Loss is not bounded between 0 and 1 like the BCE loss, the above constraint makes sure that the loss does not grow too much. \n",
    "\n",
    "In the original paper, the authors enforced this condition by using weight clipping.\n",
    "\n",
    "## WGAN-GP\n",
    "Introducing Wasserstein Gan with Gradient Penalty, or WGAN-GP. In this paper, the authors introduce a more robust way to enforce the 1-Lipschitz constaint of the critic: a **gradient penalty term** in the loss function. \n",
    "\n",
    "The gradient penalty is calculated as follows:\n",
    "* sample a random point $\\hat{x}$ between the generated distribution and the real distribution. \n",
    "* run this sample through the discriminator and calculate the gradient $\\nabla_{\\hat{x}} D(\\hat{x})$\n",
    "* calculate the L2 norm of the gradient $|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2}$\n",
    "* remove 1, square the result and calculate the mean $(|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2} - 1) ^{2}$\n",
    "\n",
    "### Second exercise [Optional]: Implement the gradient penalty\n",
    "\n",
    "In the second exercise of this notebook, you will implement the above gradient penalty. To help you, I have created a dummy critic module.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"black\">\n",
    "<b>Click for tips</b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "* To calculate the gradients, you first have to set the attribute of a tensor `requires_grad` to True.\n",
    "* You can use the following code to calculate the gradients:\n",
    "```\n",
    "torch.autograd.grad(critic(x), x, grad_outputs=torch.ones_like(critic(x)), create_graph=True)[0]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\" \n",
    "    Dummy critic class \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.pow(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(real_sample: torch.Tensor, \n",
    "                     fake_sample: torch.Tensor,\n",
    "                     critic: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gradient penalty of the WGAN-GP model\n",
    "    args:\n",
    "    - real_sample: sample from the real dataset\n",
    "    - fake_sample: generated sample\n",
    "    \n",
    "    returns:\n",
    "    - gradient penalty\n",
    "    \"\"\"\n",
    "    ####\n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sample = torch.randn(3, 32, 32)\n",
    "fake_sample = torch.randn(3, 32, 32)\n",
    "critic = Critic()\n",
    "\n",
    "gradient_penalty = gradient_penalty(real_sample, fake_sample, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAGAN\n",
    "\n",
    "The DRAGAN paper offered a different approach to calculate the gradient penalty and enforce the 1-Lipschitz constraint on the critic.\n",
    "\n",
    "(N. Kodali, J. Abernethy, et al, \"*ON CONVERGENCE AND STABILITY OF GANS*\", College of Computing Georgia Institute of Technology [Online]Available at: arxiv.org/pdf/1705.07215.pdf \\[Accessed 06/30/2022\\])\n",
    "\n",
    "<img src='assets/dragan_gp.png' width=80% />\n",
    "\n",
    "As you can see, the formula is very similar, especially since the authors use $k = 1$ for their experiments. The main difference with the WGAN-GP gradient penalty is the $\\delta$ term, which is a noise term. In their implementation, the authors calculate $X_{p} = X + \\delta $ as follows:\n",
    "\n",
    "<center>\n",
    "    $X_{p} = X + 0.5 * \\sigma({X}) * N$ \n",
    "</center>\n",
    "\n",
    "where $\\sigma$ is the standard deviation and $N$ a noise term sampled from the uniform distribution.\n",
    "\n",
    "The gradient penalty is then calculated as follows:\n",
    "* sample a random point $\\hat{x}$ between the real distribution $X$ and $X_{p}$ . \n",
    "* run this sample through the discriminator and calculate the gradient $\\nabla_{\\hat{x}} D(\\hat{x})$\n",
    "* calculate the L2 norm of the gradient $|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2}$\n",
    "* remove 1, square the result and calculate the mean $(|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2} - 1) ^{2}$\n",
    "\n",
    "\n",
    "### BCE Loss\n",
    "Interestingly, using this gradient penalty lifts some of the constraint on the BCE Loss and the authors use the above gradient penalty with the vanilla GAN losses (BCE Loss).\n",
    "\n",
    "### Third exercise  [Optional]: Implement the DRAGAN gradient penalty\n",
    "\n",
    "In the third exercise of this notebook, you will implement the DRAGAN gradient penalty. This is a one liner difference with the above implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_dragan(real_sample: torch.Tensor, critic: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gradient penalty of the WGAN-GP model\n",
    "    args:\n",
    "    - real_sample: sample from the real dataset\n",
    "    \n",
    "    returns:\n",
    "    - gradient penalty\n",
    "    \"\"\"\n",
    "    ####\n",
    "    # IMPLEMENT HERE\n",
    "    ####\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dragan_gp = gradient_penalty_dragan(real_sample, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "\n",
    "The gradient penalty terms penalize each input to the critic individually. Therefore, the critic should map a single input to a single output. However, we use some layers in the discriminator that remove this property: the BatchNormalization layers. \n",
    "\n",
    "Keep this in mind if you decide to use the gradient penalty in your project! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
