\part{RNNs and Transformers}
\chapter{Introduction to RNNs}
In this lesson, we will learn about \textbf{Recurrent Neural Networks (RNNs)}. \newline

The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous inputs when generating the current output. In other words, our systems did not have any \textbf{memory} elements. RNNs address this basic and essential issue by using \textbf{memory} (i.e., past inputs to the network) when producing the current output. \newline

\href{https://www.youtube.com/watch?v=AIQEqg6F38A&t=1s&ab_channel=Udacity}{Youtube}

Temporal dependencies = current output depends on current input and past inputs 

\includegraphics[width=0.75\linewidth]{img//rnn/nn_rnn.png}

\section{A bit of history}

How did the theory behind RNN evolve? Where were we a few years ago and where are we now? \newline

\href{https://www.youtube.com/watch?v=HbxAnYUfRnc&ab_channel=Udacity}{Youtube} \newline

As mentioned in this video, RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the "\textbf{vanishing gradient}" problem in which the contribution of information decays geometrically over time. \newline

What does this mean? \newline

As you may recall, while training our network, we use \textbf{backpropagation}. In the backpropagation process, we adjust our weight matrices using a \textbf{gradient}. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small that these continuous multiplications may cause the gradient to practically "vanish." \newline

\textbf{LSTM} is one option to overcome the Vanishing Gradient problem in RNNs. \newline

As mentioned in the video, Long Short-Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) give a solution to the vanishing gradient problem by helping us apply networks that have temporal dependencies. \newline

More information about GRUs can be found \href{https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be}{\textbf{here}}. Focus on the overview titled: \textbf{GRUs}. \newline

\subsubsection{Extra Resources}

\begin{itemize}
    \item \href{https://en.wikipedia.org/wiki/Vanishing_gradient_problem}{\textbf{Vanishing Gradient Problem}}
    \item \href{https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions}{\textbf{Geometric Series}}
    \item \href{https://en.wikipedia.org/wiki/Time_delay_neural_network}{\textbf{TDNN (Time delay neural network)}}
    \item Here is the original \href{http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract}{\textbf{Elman Network}} publication from 1990. This link is provided here as it's a significant milestone in the world of RNNs. To simplify things a bit, you can take a look at the following \href{https://en.wikipedia.org/wiki/Recurrent_neural_network\#Elman_networks_and_Jordan_networks}{\textbf{additional info}}.
    \item In this \href{http://www.bioinf.jku.at/publications/older/2604.pdf}{\textbf{LSTM}} link, you will find the original paper written by \href{https://en.wikipedia.org/wiki/Sepp_Hochreiter}{\textbf{Sepp Hochreiter}} and \href{http://people.idsia.ch/~juergen/}{\textbf{JÃ¼rgen Schmidhuber}}. Don't get into all the details just yet. We will cover all of this later!
\end{itemize}


\section{RNN Applications}

The world's leading tech companies use RNNs, particularly LSTMs, in their applications. Let's take a look at a few. \newline

\href{https://www.youtube.com/watch?v=6JbTNARuKII&ab_channel=Udacity}{Youtube}

There are so many interesting applications; let's look at a few more!

\begin{itemize}
    \item Are you into gaming and bots? Check out the \href{https://openai.com/research/dota-2}{\textbf{DotA 2 bot by Open AI}}
    \item How about \href{https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8}{\textbf{automatically adding sounds to silent movies?}}
    \item Amazon's voice-to-text using high-quality speech recognition, \href{https://aws.amazon.com/lex/faqs/}{\textbf{Amazon Lex}}.
    \item Facebook uses RNN and LSTM technologies for \href{https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/}{\textbf{building language models}}
    \item Netflix also uses RNN models - \href{https://arxiv.org/pdf/1511.06939.pdf}{\textbf{here is an interesting read}}
\end{itemize}

\section{Recurrent Neural Networks}
\href{https://www.youtube.com/watch?v=ofbnDxGSUcg&ab_channel=Udacity}{Youtube} \newline

RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of the feedforward and backpropagation steps used in the training phase.

    \includegraphics[width=0.5\linewidth]{img//rnn//intro/image1.png}

Current output \(\overline{y}\) depends not only on the current input \(\overline{x}\) but also on the memory element \(\overline{s}\) which takes into account past inputs. Example: Predicting the next word in the sentence. Which requires at the last few words rather than only the current one.

There are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:

\begin{itemize}
    \item \textbf{sequences} as inputs in the training phase, and
    \item \textbf{memory} elements
\end{itemize}
Memory is defined as the output of hidden layer neurons, which will serve as additional input to the network during the next training step.

    \includegraphics[width=1\linewidth]{img//rnn//intro/image2.png}

The first difference , is the manner by which we define our inputs and outputs. Instead of training the network using a single-input, single-output at each time step, we train with sequences since previous inputs matter. The second difference, stems from the memory elements that RNN's host. Current inputs, as well as activations of neurons serve as inputs to the next time step.

\includegraphics[width=1\linewidth]{img//rnn//intro/image3.png}

In feedforward neural networks,we saw a flow of information from the input to the output without any feedback.

\includegraphics[width=0.75\linewidth]{img//rnn//intro/image.png}

Now, that feedforward scheme changes, and includes the feedback or memory elements. We will consider memory defined, as the output of the hidden layer, which will serve as an additional input to the network at the following training step. We will no longer use H as the output of the hidden layer, but S for state, referring to a system with memory. The basic scheme of RNN is called Simple RNN, and is also known as an Elman Network. \newline

The basic three layer neural network with feedback that serve as memory inputs is called the \textbf{Elman Network} and is depicted in the following picture:

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-06-at-1.40.14-pm.png}
\captionof{figure}{Elman Network, source: Wikipedia}

\subsection{RNN - Folded and Unfolded Model}
\href{https://www.youtube.com/watch?v=wsif3p5t7CI&ab_channel=Udacity}{Youtube} \newline

In FFNN the output at any time \textit{t}, is a function of the current input and the weights. This can be easily expressed using the following equation: 
\begin{equation}
    \overline{y}_t = F(\overline{x}_t, W)
\end{equation}

We assume that the inputs are independent of each other. Therefore, there is no significance to the sequence. So we train the system by randomly drawing inputs and target pairs. \newline

In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:

\begin{equation}
    \overline{y}_t = F(\overline{x}_t, \overline{x}_{t-1}, \overline{x}_{t-2}, \overline{x}_{t-t_0}, W)
\end{equation}

This is the RNN \textbf{folded model}:

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-06-at-2.09.07-pm.png}
\captionof{figure}{The RNN folded model}

In this picture, \(\overline{x}\) represents the input vector, \(\overline{y}\) represents the output vector and \(\overline{s}\) denotes the state vector. \newline

 \(W_x\) is the weight matrix connecting the inputs to the state layer. \newline

\(W_y\) is the weight matrix connecting the state layer to the output layer. \newline

\(W_s\) represents the weight matrix connecting the state from the previous timestep to the state in the current timestep. \newline

The model can also be "unfolded in time". The \textbf{unfolded model} is usually what we use when working with RNNs. \newline

\href{https://www.youtube.com/watch?v=xLIA_PTWXog&ab_channel=Udacity}{Youtube} \newline

\includegraphics[width=01\linewidth]{img//rnn//intro/screen-shot-2017-11-06-at-2.38.51-pm.png}
\captionof{figure}{The RNN unfolded model}


\subsubsection{The Unfolded Model}

The Unfolded Model is useful for visualizing the RNN. This is because The Unfolded Model provides a more straightforward way to separate and understand the parts of the RNN. \newline

Here's how the unfolded model helps us understand RNNs:

\begin{itemize}
    \item Separates the State Vector from the Input Vector to simplify the diagram
    \item Shows how both the State Vector and the Input Vector interact with the same weights
    \item Makes it easier to understand how the State at T-1 and the Input Vector at T produce the Output at T
\end{itemize}
In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and \textbf{also} on the previous state:

\begin{equation}
    \overline{s}_t = \Phi(\overline{x}_t W_x + \overline{s}_{t-1} W_s)
\end{equation}

The output vector is calculated exactly the same as in FFNNs. It can be a linear combination of the inputs to each output node with the corresponding weight matrix \(W_y\), or a softmax function of the same linear combination.

\begin{equation}
    \begin{split}
        \overline{y}_t = \overline{s}_t W_y \\
        \textnormal{or} \\
        \overline{y}_t = \sigma(\overline{s}_t W_y)
    \end{split}  
\end{equation}

\section{RNN Example}

In this example, we will illustrate how RNNs can help detect sequences. When detecting a sequence, the system must remember the previous inputs, so it makes sense to use a recurrent network. \newline

If you are unfamiliar with sequence detection, the idea is to see if a specific pattern of inputs has entered the system. In our example, the pattern will be the word U,D,A,C,I,T,Y. \newline

\href{https://www.youtube.com/watch?v=MDLk3fhpTx0&ab_channel=Udacity}{Youtube} 


\section{Backpropagation Through Time - I}

We are now ready to understand how to train the RNN.

When we train RNNs we also use backpropagation, but with a conceptual change. The process is similar to that in the FFNN, with the exception that we need to consider previous time steps, as the system has memory. This process is called \textbf{Backpropagation Through Time (BPTT)}.

We will use the \href{https://en.wikipedia.org/wiki/Mean_squared_error}{\textbf{MSE loss function}} to explain the BPTT. Recall that Mean Squared error \(E_t = (\overline{d}_t - \overline{y}_t)^2\)

where
\begin{itemize}
     \item \(E_t\)represents the output error at time t
     \item \(d_t\) represents the desired output at time t
     \item \(y_t\) represents the calculated output at time t
\end{itemize}
\href{https://www.youtube.com/watch?v=eE2L3-2wKac&ab_channel=Udacity}{Youtube} \newline

In \textbf{BPTT} we train the network at timestep t and take into account all of the previous timesteps. Let's understand this with the help of an example. \newline

In this example, we will focus on the \textbf{BPTT} process for time step t=3. You will see that to adjust all three weight matrices, \(W_x, W_s,\) and \(W_y\), we need to consider timestep 3 as well as timestep 2 and timestep 1. \newline

As we are focusing on timestep t=3, the Loss function will be:  \(E_3 = (\overline{d}_3 - \overline{y}_3)^2\)

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-27-at-1.43.36-pm.png}
\captionof{figure}{The Folded Model at Timestep 3}

To update each weight matrix, we need to find the partial derivatives of the Loss Function at time 3, as a function of all of the weight matrices. We will modify each matrix using gradient descent while considering the previous timesteps.

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-27-at-1.46.43-pm.png}
\captionof{figure}{Gradient Considerations in the Folded Model}

We will now unfold the model. You will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time process. These multiplications stem from the chain rule and are easily visualized using this model.


\section{Backpropagation Through Time - II}

\subsubsection{Unfolding the model in time}

In the following video we will understand how to use Backpropagation Through Time (BPTT) when adjusting two weight matrices:

\begin{itemize}
    \item \(W_y\)- the weight matrix connecting the state of the output
    \item \(W_s\) - the weight matrix connecting one state to the next state
\end{itemize}
\href{https://www.youtube.com/watch?v=bUU9BEQw0IA&ab_channel=Udacity}{Youtube} \newline



\textbf{Gradient calculations needed to adjust \(W_y\)} \newline

The partial derivative of the Loss Function concerning \(W_y\) is found by a simple one-step chain rule: (Note that in this case, we do not need to use BPTT. Visualization of the calculations path can be found in the video).

\[\frac{\partial E_3}{\partial W_y} = \frac{\partial E_3}{\partial \overline{y}_3} \frac{\partial \overline{y}_3}{\partial W_y}\]
\captionof{figure}{Chain rule for calculating partial derivative of loss}

\[\frac{\partial E_N}{\partial W_y} = \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial W_y}\]
\captionof{figure}{Chain rule for calculating partial derivative of loss (N steps)}

\textbf{Gradient calculations needed to adjust \(W_s\)} \newline

We still need to adjust \(W_s\) the weight matrix connecting one state to the next and \(W_x\) the weight matrix connecting the input to the state. We will arbitrarily start with \(W_s\). \newline

To understand the \textbf{BPTT} process, we can simplify the unfolded model. We will focus on the contributions of \(W_s\) to the output, the following way:

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-27-at-2.00.15-pm.png}
\captionof{figure}{Simplified Unfolded model for Adjusting Ws}

When calculating the partial derivative of the Loss Function for \(W_s\), we need to consider all of the states contributing to the output. In the case of this example, it will be states \(\overline{s}_3\) which depends on its predecessor \(\overline{s}_2\) which depends on its predecessor \(\overline{s}_1\), the first state. \newline

In \textbf{BPTT}, we will consider every gradient stemming from each state, \textbf{accumulating} all of these contributions. \newline

At timestep t=3, the contribution to the gradient stemming from \(\overline{s}_3\), \(\overline{s}_2\), and \(\overline{s}_1\) is the following: \newline

\textbf{Note: Notice the use of the chain rule here. If you need, go back to the video to visualize the calculation path.}

\includegraphics[width=0.5\linewidth]{img//rnn//intro/bptt_eq.jpeg}
\captionof{figure}{Final gradient calculation in the BPTT algorithm (considering 3 timesteps)}

Generalizing for N timesteps, we get: \[\frac{\partial E_N}{\partial W_s} = \sum_{i=1}^N \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial \overline{s}_i} \frac{\partial \overline{s}_i}{\partial W_s}\] \captionof{figure}{Final gradient calculation in the BPTT algorithm (considering N timesteps)}

We still need to adjust \textbf{\(W_x\)}, the weight matrix connecting the nuput to the state.

\subsubsection{Adjusting / Updating \(W_x\)}
\href{https://www.youtube.com/watch?v=uBy_eIJDD1M&ab_channel=Udacity}{Youtube} \newline

When calculating the partial derivative of the Loss Function concerning to \(W_x\) we need to consider, again, all of the states contributing to the output. As we saw before, in the case of this example, it will be states \(\overline{s}_3\), which depend on its predecessor \(\overline{s}_2\), which depends on its predecessor \(\overline{s}_1\), the first state.

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-08-at-3.43.34-pm.png}
\captionof{figure}{Simplified Unfolded model for Adjusting Wx}

After considering the contributions from all three states: \(\overline{s}_3\), \(\overline{s}_2\) and \(\overline{s}_1\), we will \textbf{accumulate} them to find the final gradient calculation. \newline

The following equation is the gradient contributing to the adjustment of \textbf{\(W_x\) }using \textbf{Backpropagation Through Time}:

\begin{equation}
    \begin{split}
        \frac{\partial E_3}{\partial W_x} & = \frac{E_3}{\partial \overline{y}_3} \frac{\partial \overline{y}_3}{\partial \overline{s}_3} \frac{\partial \overline{s}_3}{\partial W_x} + \\
        & = \frac{E_3}{\partial \overline{y}_3} \frac{\partial \overline{y}_3}{\partial \overline{s}_3} \frac{\partial \overline{s}_3}{\partial \overline{s}_2} \frac{\partial \overline{s}_2}{\partial W_x} + \\
        & = \frac{E_3}{\partial \overline{y}_3} \frac{\partial \overline{y}_3}{\partial \overline{s}_3} \frac{\partial \overline{s}_3}{\partial \overline{s}_2} \frac{\partial \overline{s}_2}{\partial \overline{s}_1} \frac{\partial \overline{s}_1}{\partial W_x}
    \end{split}
\end{equation}
\captionof{figure}{Equation for Updating Wx (considering 3 timesteps)}

\includegraphics[width=1\linewidth]{img//rnn//intro/image_backpropagation_t3.png}

Generalizing for N timesteps, we get: \[\frac{\partial E_N}{\partial W_x} = \sum_{i=1}^N \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial \overline{s}_i} \frac{\partial \overline{s}_i}{\partial W_x}\] \captionof{figure}{Equation for updating Wx (considering N timesteps)}

\section{BPTT Quizzes}

\subsection{Quiz 1}

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-11-29-at-3.08.28-pm.png}

\textit{A folded RNN model} \newline

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-29-at-5.33.53-pm.png}

\textbf{Quiz Question} \newline

Consider the above folded RNN Model. Both states \textbf{S} and \textbf{Z} have multiple neurons in each layer. The mathematical derivation of state \textbf{Z} at time t is:
\begin{itemize}
    \item Equation A
    \item Equation B
    \item Equation C
    \item \textbf{Equation D}
\end{itemize}
\textbf{Solution}: \(\overline{z}\) and \(\overline{s}\) are vectors, as we indicate that they have multiple neurons in each layer. Using this logic we can understand that equations A and C are incorrect. Since \(w_2\) connects the hidden state \(\overline{z}\) to itself, we know that we need to consider the previous timestep here. Therefore only equation D is the correct one.

\subsection{Quiz 2}
\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-11-29-at-3.08.28-pm.png}

\textit{A folded RNN model} \newline
\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-29-at-5.33.53-pm.png}

\textbf{Quiz Question} \newline

Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol \textbf{E}. What is the update rule of weight matrix V1 at time t, over a single timestep ?
\begin{itemize}
    \item Equation A
    \item \textbf{Equation B}
    \item Equation C
    \item Equation D
\end{itemize}

\subsection{Quiz 3}

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-11-29-at-3.08.28-pm.png}

\textit{A folded RNN model} \newline
\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-12-04-at-12.10.02-pm.png}

\textbf{Quiz Question} \newline

Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol E. What is the update rule of weight matrix U at time t+1 (over 2 timesteps) ? Hint: Use the unfolded model for a better visualization.
\begin{itemize}
    \item Equation A
    \item Equation B
    \item \textbf{Equation C}
\end{itemize}
\textbf{Solution}: To understand how to update weight matrix U, we will need to unfold the model in time. We will unfold the model over two time steps, as we need to look only time t and time t+1. The following three pictures will help you understand the \textbf{three} paths we need to consider. Notice that we have two hidden layers that serve as memory elements, so this case will be different than the one we saw in the video, but the idea is the same. We will use \textbf{BPTT} while applying the chain rule.

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-12-04-at-11.16.19-am.png}
\captionof{figure}{The first path to consider}

The following is the equation we derive using the first path: \[\frac{\partial E_{t + 1}}{\partial U} = \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{s}_{t+1}} \frac{\partial \overline{s}_{t+1}}{\partial U}\]

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-12-04-at-11.14.30-am.png}
\captionof{figure}{The second path to consider}

The following is the equation we derive using the second path: \[\frac{\partial E_{y+1}}{\partial U} = \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{s}_{t+1}} \frac{\partial \overline{s}_{t+1}}{\partial \overline{s}_t} \frac{\partial \overline{s}_t}{\partial U}\]

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-12-04-at-11.12.31-am.png}
\captionof{figure}{The third path to consider}

The following is the equation we derive using the third path: \[\frac{\partial E_{t+1}}{\partial U} = \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{z}_{t}} \frac{\partial \overline{z}_{t}}{\partial \overline{s}_{t}} \frac{\partial \overline{z}_{t}}{\partial U}\]

Finally, after considering all three paths, we can derive the correct equation for the purposes of updating weight matrix U, using BPTT:
\begin{equation}
    \begin{split}
        \frac{\partial E_{t+1}}{\partial U} = & \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{s}_{t+1}} \frac{\partial \overline{s}_{t+1}}{\partial U} +\\
        & \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{s}_{t+1}} \frac{\partial \overline{s}_{t+1}}{\partial \overline{s}_t} \frac{\partial \overline{s}_t}{\partial U} +\\
        & \frac{\partial E_{t+1}}{\partial \overline{y}_{t+1}} \frac{\partial \overline{y}_{t+1}}{\partial \overline{z}_{t+1}} \frac{\partial \overline{z}_{t+1}}{\partial \overline{s}_{t+1}} \frac{\partial \overline{s}_{t+1}}{\partial \overline{s}_t} \frac{\partial \overline{s}_t}{\partial U}
    \end{split}
\end{equation}

\section{RNN Theory Summary}

Let's summarize what we have seen so far: \href{https://www.youtube.com/watch?v=nXP0oGGRrO8&ab_channel=Udacity}{Youtube} \newline

As you have seen, in RNNs the current state depends on the input and the previous states, with an activation function. \[\overline{s}_t = \Phi (\overline{x}_t W_x + \overline{s}_{t-1} W_s)\] \captionof{figure}{The current state as a function of input and the previous state}

The current output is a simple linear combination of the current state elements with the corresponding weight matrix. Equation showing the current output:
\begin{itemize}
    \item \(\overline{y}_t = \overline{s}_t W_y\) (without the use of an activation function), or
    \item \(\overline{y}_t = \sigma(\overline{s}_t W_y)\) (withthe use of an activation function)
\end{itemize}
We can represent the recurrent network with the use of a folded model or an unfolded model:

\includegraphics[width=0.25\linewidth]{img//rnn//intro/screen-shot-2017-11-27-at-2.44.11-pm.png}
\captionof{figure}{The RNN Folded Model}

\includegraphics[width=0.5\linewidth]{img//rnn//intro/screen-shot-2017-11-27-at-3.48.31-pm.png}
\captionof{figure}{The RNN Unfolded Model}

We will have three weight matrices to consider in the case of a single hidden (state) layer. Here we use the following notations:\newline

\(W_x\) - represents the weight matrix connecting the inputs to the state layer.\newline

\(W_y\) - represents the weight matrix connecting the state to the output. \newline

\(W_s\) - represents the weight matrix connecting the state from the previous timestep to the state in the following timestep. \newline

The gradient calculations for the purpose of adjusting the weight matrices are the following: \[\frac{\partial E_N}{\partial W_y} = \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial W_y}\] \captionof{figure}{Equation 1}

\[\frac{\partial E_N}{\partial W_s} = \sum_{i=1}^N \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial \overline{s}_i} \frac{\partial \overline{s}_i}{\partial W_s}\] \captionof{figure}{Equation 2}

\[\frac{\partial E_N}{\partial W_x} = \sum_{i=1}^N \frac{\partial E_N}{\partial \overline{y}_N} \frac{\partial \overline{y}_N}{\partial \overline{s}_i} \frac{\partial \overline{s}_i}{\partial W_x}\] \captionof{figure}{Equation 3}

When training RNNs using BPTT, we can choose to use mini-batches, where we update the weights in batches periodically (as opposed to once every inputs sample). We calculate the gradient for each step but do not update the weights immediately. Instead, we update the weights once every fixed number of steps. This helps reduce the complexity of the training process and helps remove noise from the weight updates. \newline

The following is the equation used for \textbf{Mini-Batch Training Using Gradient Descent}: (where \(\delta_{ij}\) represents the gradient calculated once every inputs sample, and M represents the number of gradients we accumulate in the process). \[\delta_{ij} = \frac{1}{M}\sum_{k=1}^M \delta_{ij_k}\] \captionof{figure}{Equation 4}

If we backpropagate more than \~10 timesteps, the gradient will become too small. This phenomenon is known as the \textbf{vanishing gradient problem}, where the contribution of information decays geometrically over time. Therefore, the network will effectively discard temporal dependencies that span many time steps. \textbf{Long Short-Term Memory (LSTM)} cells were designed to solve this problem specifically. \newline

In RNNs we can also have the opposite problem, called the \textbf{exploding gradient} problem, in which the value of the gradient grows uncontrollably. A simple solution for the exploding gradient problem is \textbf{Gradient Clipping}. \[\delta = \frac{\partial y}{\partial W_{ij}}\]
At each timestep t, we check if \(\delta\) exceeds a threshold. If it does, we normalize the gradient. Normalizing means here that we penalize very large gradients more than those gradients that are slightly larger than the threshold. \newline

More information about Gradient Clipping can be found \href{https://arxiv.org/abs/1211.5063}{\textbf{here}}. \newline

You can concentrate on Algorithm 1 which describes the gradient clipping idea in simplicity.

\section{Implementing RNNs}
\href{https://www.youtube.com/watch?v=BHoiwB61ays&ab_channel=Udacity}{Youtube} \newline

RNNs are very useful for sequence-based tasks. The challenges for implementing RNNs are two-fold.

\begin{itemize}
    \item How do we pre-process sequential data for our network?
    \item How do we represent memory in our code?
\end{itemize}
\input{rnn/simpleRNN_predict_timeseries_data}
\input{rnn/simpleRNN_predict_household_power_consumption}

\section{Dealing with textual data}

So far, we have learned how to implement RNNs for predicting time series and have dealt with numeric data only. However, when it comes to using RNNs with textual data, there are some fundamental differences that need to be addressed.

\begin{itemize}
    \item Textual data is \textbf{unstructured}: Unlike numeric data, textual data is unstructured and cannot be directly used as input to a neural network. Before we can use RNNs for tasks like sentiment analysis or question answering, we need to preprocess the text data and convert it into a structured format.
    \item \textbf{Vocabulary size}: Textual data typically has a larger vocabulary size compared to numeric data. For example, the English language has over 170,000 words. To process text data, we need to convert words into a numerical representation that can be understood by the neural network.
    \item Handling \textbf{variable-length input}: Textual data is variable-length, meaning that different text inputs can have different lengths. This makes it challenging to use traditional neural networks that require fixed input sizes. To handle variable-length inputs, we need to use techniques like padding to ensure that all inputs are of the same length.
    \item \textbf{Semantic representation}: Unlike numeric data, where each value has a clear meaning, words in text data can have different meanings depending on their context. For example, the word "bank" could refer to a financial institution or the edge of a river. To ensure that the neural network can understand the meaning of words in a given context, we need to use techniques like \textit{word embeddings} that represent each word as a vector of continuous values that capture its semantic meaning.
\end{itemize}
By learning techniques like \textit{tokenization}, \textit{stopword removal}, \textit{stemming} and \textit{lemmatization}, encoding techniques like \textit{bag of words}, \textit{TF-IDF}, and \textit{word embeddings}, and \textit{padding}, you would be able to preprocess textual data and use RNNs to perform tasks like sentiment analysis, question answering, text generation, etc.

Let's understand some of these techniques for preprocessing texts.

\subsection{Normalization}
\href{https://www.youtube.com/watch?v=eOV2UUY8vtM&ab_channel=Udacity}{Youtube} \newline

Plain text is great but it's still human language with all its variations and bells and whistles. We try to reduce some of that complexity in the English language. Lowercase conversion and punctuation removal are the two most common text normalization steps. Whether you need to apply them and at what stage depends on your end goal and the way you
design your pipeline

\subsubsection{Case normalization}

The starting letter of the first word in any sentence is usually capitalized, all caps are sometimes used for emphasis and for stylistic reasons. While this is convenient for a human reader from the standpoint of a ML algorithm, it does not make sense to differentiate between Car, car, and CAR. They all mean the same thing. Therefore, we usually convert every letter in our text to a common case usually lowercase so that each word is represented by a unique token. 
\begin{lstlisting}
    # Convert to lowercase
    text = text.lower()
    print(text)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/case_normalization.png}

\subsubsection{Punctuation removal}
Depending on your NLP task you may want to remove special characters like periods, question marks, and exclamation points from the text and only keep letters of the alphabet and maybe numbers. This is especially useful when we are looking at text documents as a whole in applications like document classification and clustering where the low-level details do not matter a lot. 

\begin{lstlisting}
    import re
    # Remove punctuation characters
    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)
    print(text)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/punctuation_removal.png}

\subsection{Tokenization}
\href{https://www.youtube.com/watch?v=4Ieotbeh4u8&t=1s&ab_channel=Udacity}{Youtube}\newline

Token is a fancy term for a symbol usually one that holds some meaning and is not typically split up any further. In case of natural language processing our tokens are usually individual words. So, tokenization is simply splitting each sentence into a sequence of word. 

\subsubsection{Whitespace Tokenization}

The simplest way to do this is using the split method which returns a list of words.

\begin{lstlisting}
    # Split text into tokens (words)
    words = text.split()
    print(words)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/whitespace_tokenization.png}

Note that it splits on whitespace characters by default which includes regular spaces but also tabs, new lines, etc.

\subsubsection{Word Tokenization}

The library \lstinline{nltk}, which stands for natural language toolkit. Its \lstinline{word\_tokenize()} function performs the same task as split but is a little smarter. 

\begin{lstlisting}
    from nltk.tokenize import word_tokenize
    # Split text into words using NLTK
    words = words_tokenize(text)
    print(words)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/word_tokenization.png}

Notice that the punctuation are treated differently based on their position:here the period after the title doctor has been retained along with D R as a single token. 

\subsubsection{Sentence Tokenization}
Sometimes you may need to split text into sentences. For instance if you want to translate it.

\begin{lstlisting}
    from nltk.tokenize import sent_tokenize
    # Split text into sentences
    sentences = sent_tokenize(text)
    print(sentences)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/sentence_tokenization.png}

NLTK provides several other tokenizers, including a regular expression based organizer that you can use to remove punctuation and perform tokenization in a single step and also a tweet tokenizer that is aware of Twitter handles, hashtags ,and emoticons.

\subsection{Stop Word Removal}
\href{https://www.youtube.com/watch?v=WAU_Ij0GJbw&t=3s&ab_channel=Udacity}{Youtube}\newline

Stop words are uninformative words like is, are, the, in, at, etc. that do not add a lot of meaning to a sentence. They are
typically very commonly occurring words and we may want to remove them to reduce the vocabulary we have to deal with and hence the complexity of later procedures. \newline

You can see for yourself which words NLTK considers to be stop words in English. Note that this is based on a specific corpus or collection of text. Different corpora may have different stop words.

\begin{lstlisting}
    # List stop words
    from nltk.corpus import stopwords
    print(stopwords.words('english'))
\end{lstlisting}

\begin{lstlisting}
    # Remove stop words
    words = [w for w in words if w not in stopwords.words('english')]
    print(words)
\end{lstlisting}

\subsection{Stemming and Lemmatization}
\href{https://www.youtube.com/watch?v=7Gjf81u5hmw&t=1s&ab_channel=Udacity}{Youtube}

Stemming is the process of reducing a word to its stem or root form, e.g. branching, branched, branches, etc can all be reduced to branch after all they convey the idea of something separating into multiple paths or branches again this helps reduce complexity while retaining the essence of meaning that is carried by words.

Stemming is meant to be a fast and crude operation carried out by applying very simple search and replace style rules. NLTK has a few different stammers for you to choose from

\begin{lstlisting}
    from nltk.stem.porter import PorterStemmer

    # Reduce words to their stems
    stemmed = [PorterStemmer().stem(w) for w in words]
    print(stemmed)
\end{lstlisting}



for example the suffixes I and G and E D
can be dropped off IES can be replaced
by Y etc this may result in stem words
that are not complete words but that's
okay as long as all forms of that word
are reduced to the same stem thus
capturing the common underlying idea including porter
stemmer that we use here
snowball stammer and other language
specific stammers you simply need to
pass in one word at a time
note that here we have already removed
stop words some of the conversions are
actually pretty good like started
reduced to start others like people
losing the e at the end are a result of
applying very simplistic rules 

Lemmatization is another technique used to reduce words to a normalized form but in this case the transformation actually uses a dictionary to map different variants of a word back to its root. With this approach we are able to reduce non-trivial inflections such as \textit{is, was, were} back to the root \textit{be}. 

\begin{lstlisting}
    from nltk.stem.wordnet import WordNetLemmatizer
    # Reduce words to their root form
    lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]
    print(lemmed)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/lemmatizer.png}

Only the word \textit{ones} got reduced to \textit{one}, all the others are unchanged. If you read the words carefully, you'll see that \textit{once} is the only plural noun here. 

\begin{lstlisting}
    # Lemmatize verbs by specifying pos
    lemmed = [WordNetLemmatizer().lemmatize(w, pos = 'v') for w in lemmed]
    print(lemmed)
\end{lstlisting}

\includegraphics[width=1\linewidth]{img//rnn//intro/lemmitization2.png}

Note that there are other verbs but they're already in the root form. \newline

Lemmatization is similar to stemming with one difference: the final form is also a meaningful word. That said stemming does not need a dictionary like lemmatization does so
depending on the constraints you have stemming may be a less memory intensive option for you to consider.

\input{rnn/text_preprocessing}


\section{Word Embeddings}
\href{https://www.youtube.com/watch?v=QFQtwraClgk&t=1s&ab_channel=Udacity}{Youtube}

\subsection{What are word embeddings?}

Word embeddings are a type of distributed representation used in natural language processing (NLP) that allow words to be represented as dense vectors of real numbers. Each word is mapped to a unique vector, and the vector space is designed such that words that are semantically similar are located close to each other in the vector space. \newline

Word embeddings are typically learned through unsupervised learning techniques, such as neural network models like \href{https://arxiv.org/pdf/1301.3781.pdf}{\textbf{Word2Vec}} and \href{https://nlp.stanford.edu/pubs/glove.pdf}{\textbf{GloVe}}, which are trained on large corpora of text. During training, the model learns to predict the context in which a word appears, such as the surrounding words in a sentence, and uses this information to assign a vector representation to each word.

\subsection{Why word embeddings are important?}

Word embeddings have revolutionized the field of natural language processing by providing a way to represent words as dense vectors that capture semantic and syntactic relationships between words. These representations are particularly useful for downstream NLP tasks, such as text classification, sentiment analysis, and machine translation, where traditional techniques may struggle to capture the underlying structure of the text. \newline

For example, in a sentiment analysis task, word embeddings can be used to capture the sentiment of a sentence by summing the vector representations of the words in the sentence and passing the result through a neural network. In a machine translation task, word embeddings can be used to map words from one language to another by finding the closest vector representation in the target language.

\subsection{Models for word embedding in Pytorch?}

Pytorch provides several models for word embedding, including:

\begin{quote}
Note: Please refer to the research papers if you want to understand these models in detail. Our aim is to introduce these models and see how to use them in practice.

\end{quote}

\textbf{GloVe (Global Vectors)}: It is a method for generating word embeddings, which are dense vector representations of words that capture their semantic meaning. The main idea behind GloVe is to use co-occurrence statistics to generate embeddings that reflect the words' semantic relationships. GloVe embeddings are generated by factorizing a co-occurrence matrix. The co-occurrence matrix is a square matrix where each row and column represents a word in the vocabulary, and the cell at position (i, j) represents the number of times word i and word j appear together in a context window. The context window is a fixed-size window of words surrounding the target word. The factorization of the co-occurrence matrix results in two smaller matrices: one representing the words, and the other representing the contexts. Each row of the word matrix represents a word in the vocabulary, and the entries in that row are the weights assigned to each dimension of the embedding. Similarly, each row of the context matrix represents a context word, and the entries in that row are the weights assigned to each dimension of the context embedding. The GloVe embeddings are computed by multiplying the word and context embeddings together and summing them up. This produces a single scalar value that represents the strength of the relationship between the two words. The resulting scalar is used as the value of the (i, j) entry in the word-context co-occurrence matrix. In PyTorch, you can use the torchtext package to load pre-trained GloVe embeddings. The \verb|torchtext.vocab.GloVe| class allows you to specify the dimensionality of the embeddings (e.g. 50, 100, 200, or 300), and the pre-trained embeddings are downloaded automatically. \newline

\textbf{FastText}: FastText is a popular method for generating word embeddings that extends the concept of word embeddings to subword units, rather than just whole words. The main idea behind FastText is to represent each word as a bag of character n-grams, which are contiguous sequences of n characters. FastText embeddings are generated by training a shallow neural network on the subword units of the corpus. The input to the network is a bag of character n-grams for each word in the vocabulary, and the output is a dense vector representation of the word. During training, the network uses a negative sampling objective to learn the embeddings. The objective is to predict whether or not a given word is in the context of a target word. The model learns to predict the context of a word by computing the dot product between the target word's embedding and the embedding of each subword unit in the context. FastText embeddings have several advantages over traditional word embeddings. For example, they can handle out-of-vocabulary words, as long as their character n-grams are present in the training corpus. They can also capture morphological information and handle misspellings, since they are based on subword units. In PyTorch, you can use the torchtext package to load pre-trained FastText embeddings. The \verb|torchtext.vocab.FastText| class allows you to specify the language and the dimensionality of the embeddings (e.g. 300). \newline

\textbf{CharNgram}: It refers to a method of generating character-level embeddings for words. The idea behind charNgram is to represent each word as a sequence of character n-grams (substrings of length n), and then use these n-grams to generate a fixed-length embedding for the word. For example, if we use CharNGram with n=3, the word "hello" would be represented as a sequence of 3-character n-grams: "hel", "ell", "llo". We would then use these n-grams to generate a fixed-length embedding for the word "hello". This embedding would be a concatenation of the embeddings of each n-gram. The benefit of using charNgram embeddings is that they can capture information about the morphology of words (i.e. how the word is formed from its constituent parts), which can be useful for certain NLP tasks. However, charNgram embeddings may not work as well for tasks that rely heavily on semantic meaning, since they do not capture the full meaning of a word. In PyTorch, you can generate charNgram embeddings using the torchtext package. The \verb|torchtext.vocab.CharNGram| class allows you to generate character n-grams for a given text corpus, and the resulting n-grams can be used to generate charNgram embeddings for individual words. \newline

\textbf{BERT}: a pre-trained language model that can be fine-tuned for various downstream NLP tasks and also produces high-quality word embeddings. Pytorch provides pre-trained BERT models through the \href{https://pytorch.org/hub/huggingface_pytorch-transformers/}{\textbf{transformers}} library.
In addition to these pre-trained models, Pytorch provides tools for training your own word embeddings from scratch. Take a look at \textbf{Word Embeddings in PyTorch} below.

\subsubsection{Additional Materials:}

\begin{itemize}
    \item \href{https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\#word-embeddings-in-pytorch}{\textbf{Word Embeddings in PyTorch}}
    \item \href{https://nlp.stanford.edu/projects/glove/}{\textbf{GloVe}}
    \item \href{https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2761021.pdf}{\textbf{Word2Vec using Character n-grams}}
    \item \href{https://fasttext.cc/docs/en/crawl-vectors.html}{\textbf{FastText}}
\end{itemize}

\section{Implementing Word Embeddings}
\input{rnn/embeddings}
