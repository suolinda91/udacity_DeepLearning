\chapter{Introduction to Transformers}

\section{So what are transformer models and why they are taking the world by storm?}

Originally introduced in 2017 by Google researchers led by Ashish Vaswani, transformer models are a type of neural network architecture. They are designed to process sequential data (e.g., words in a sentence), such as natural language text. But here is why transformer models are revolutionary - they use a \textbf{self-attention} mechanism.\newline

This \textbf{self-attention mechanism} allows them to \textbf{focus on different parts} \textbf{of the input sequence} and \textbf{adjust} \textbf{their importance} when making predictions about the output. In contrast Recurring Neural Networks (RNNs)/ Long Short-Term Memory (LSTM)/ Gated recurrent units (GRUs) are other types of Neural Networks that process \textit{a sequence one element at a time}. Unlike self-attention, RNNs process the sequence in a linear fashion, with each element being processed sequentially based on its position in the sequence. As a result, these have a limited attention span and cannot “remember” the context from an earlier part of the sequence or conversation. Let’s see this with a visual. \newline

\href{https://www.youtube.com/watch?v=6c4bkZyztSA&ab_channel=Udacity}{Youtube} 

\subsection{Summary}

So while LSTMs have been very effective in handling sequential data, they do have some limitations:

\begin{enumerate}
    \item Limited attention span - They struggle to capture long term dependencies in sequences as they maintain a limited amount of information in memory.
    \item Computation efficiency - LSTMs are computationally expensive to train.
    \item Handling multiple sequences - LSTMs are designed to handle one sequence at a time.
\end{enumerate}
Transformers overcome all these limitations of LSTM by using \textbf{self-attention} and \textbf{parallel processing}. \newline

Transformer models have been shown to achieve state-of-the-art performance on a wide range of NLP tasks, including:

\begin{itemize}
    \item language translation
    \item text generation
    \item question answering
    \item sentiment analysis
    \item named-entity recognition
\end{itemize}
This has led to their widespread adoption in industry and academia, and they are now the dominant approach for many NLP applications. Their impact has been particularly significant in the development of large-scale language models, such as Bidirectional Encoder Representation Transformer (BERT), and Generative Pre-trained Transformer (GPT), which have revolutionized the field of NLP across a wide range of tasks.

\subsection{Open-source APIs for Transformers}

The availability of these powerful transformer models can be found in numerous open-source APIs are currently accessible from various companies, including OpenAI, TensorFlow Hub, AWS, Google Cloud AI Platform, and Hugging Face Transformers. These APIs offer convenient integration into the data pipelines of businesses, allowing them to take advantage of pre-existing transformer models in deep learning and data science. \newline

If you feel like doing a quick check on testing your understanding, do check out the quiz below.

\subsection{Quiz Question}

You are working on a natural language processing project and you are considering using a transformer model for your task. What is multi-head attention and how does it help improve the performance of a transformer model?

\begin{itemize}
    \item Multi-head attention is the ability of the transformer model to process multiple input sequences simultaneously, allowing it to handle longer sequences more effectively.
    \item \textbf{Multi-head attention is a technique for allowing the model to focus on different parts of the input sequence at different levels of abstraction, allowing it to capture more complex relationships between the words.}
    \item Multi-head attention is a mechanism for combining information from different layers of the model, allowing it to leverage information from multiple levels of abstraction.
\end{itemize}

Multi-head attention is a mechanism for allowing the model to focus on different parts of the input sequence at different levels of abstraction. This can help it capture more complex relationships between words in a sentence. It will allow the model to attend to multiple parts of the input sequence simultaneously, so multi-head attention can help it handle longer sequences more effectively.

\section{Transformers in detail}

\subsection{Transformer Architecture}
\includegraphics[width=1\linewidth]{img//rnn//transformers/transformer_architecture.jpeg}
\captionof{figure}{Transformer - model architecture [Vaswani et al. (2017). \textit{Attention is all you need.}]}

Transformers are a type of deep learning architecture that has become increasingly popular in natural language processing (NLP) tasks such as language translation and text generation. Transformers were introduced in a 2017 paper titled "Attention Is All You Need" by Vaswani et al., and have since become a cornerstone of many state-of-the-art NLP models. \newline

At a high level, the transformer architecture consists of an \textit{encoder} and a \textit{decoder}.

\begin{itemize}
    \item The encoder takes in a sequence of input tokens and produces a sequence of hidden representations
    \item The decoder takes in the encoder's output and generates a sequence of output tokens.
\end{itemize}
The key innovation of transformers is the use of \textit{self-attention} mechanisms, which allow the model to selectively focus on different parts of the input sequence when computing the hidden representations. \newline

The self-attention mechanism works by computing attention weights between each input token and all other input tokens and using these weights to compute a weighted sum of the input token embeddings. The attention weights are computed using a softmax function applied to the dot product of a \textit{query} vector, a \textit{key} vector, and a scaling factor. The query vector is derived from the previous layer's hidden representation, while the key and value vectors are derived from the input embeddings. The resulting weighted sum is fed into a multi-layer perceptron (MLP) to produce the next layer's hidden representation.\newline

More specifically, given an input sequence of length L, the encoder can be represented by a series of L identical layers, each consisting of a self-attention mechanism and a feedforward neural network:

\includegraphics[width=1\linewidth]{img//rnn//transformers/eq1.jpeg}
\captionof{figure}{Encoder equation}

\href{https://www.youtube.com/watch?v=F-XN72bQiMQ&ab_channel=Udacity}{Youtube}
\subsubsection{Key, Value, and Query}

Let's try to understand \textit{the Key}, \textit{Value}, and \textit{Query} before discussing the Decoder.

The key, value, and query vectors are used in the self-attention mechanism to help the model selectively attend to different parts of the input sequence.

\begin{itemize}
    \item \textbf{Key:} You can think of the key vectors as a set of reference points the model uses to decide which parts of the input sequence are important.
    \item \textbf{Value}: The value vectors are the actual information that the model associates with each key vector.
    \item \textbf{Query}: Query vectors are used to determine how much attention to give to each key-value pair.
\end{itemize}

\begin{quote}
Example: imagine you are trying to summarize a long article. The key vectors could represent the most important sentences or phrases in the article, while the value vectors could represent the actual content of those sentences. The query vectors would then be used to decide which of these key-value pairs are most relevant to the task of summarization.

\end{quote}

The self-attention mechanism works by computing a dot product between the query vector and each key vector, which produces a set of attention weights that indicate how much attention to give to each value vector. The resulting weighted sum of the value vectors represents the attended information for that particular query.

In summary, key, value, and query vectors are used in transformers to help the model focus on important parts of the input sequence and produce more accurate and relevant output.

\subsubsection{The Mathematics behind Transformers}

The mathematics behind transformers can be quite complex, but at a high level, it involves matrix multiplications, dot products, and non-linear activations. The key equations for the self-attention mechanism can be expressed as follows:
\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]
where Q, K, and V are the query, key, and value matrices, respectively, and \(d_k\) is the dimension of the key vectors. The softmax function is applied row-wise to the dot product of Q and K, which produces a set of attention weights that are used to weight the values in V. The output of the self-attention mechanism is then given by: \[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\]

\subsubsection{Decoder}

The decoder is similar to the encoder but also includes an additional attention mechanism that allows it to attend to the encoder's output.

Overall, the transformer architecture has several advantages over previous NLP models. First, it is highly parallelizable, which makes it more efficient to train on modern hardware. Second, it does not rely on any explicit notion of sequence order, which allows it to better capture long-term dependencies in the input sequence. Finally, the attention mechanisms allow the model to selectively attend to different parts of the input sequence, which helps it handle tasks such as language translation where the input and output sequences may have different lengths.

\section{HuggingFace}

\begin{itemize}
    \item Hugging Face is an open-source company that provides NLP tools and models for developers and researchers. Learn more at their \href{https://huggingface.co/}{\textbf{website}}.
    \item Their flagship product is the Hugging Face Transformers library, which is a Python-based framework for building, training, and deploying state-of-the-art NLP models. Explore the library on \href{https://github.com/huggingface/transformers}{\textbf{GitHub}}
    \item Hugging Face Transformers provides pre-trained models for a variety of NLP tasks, such as text classification, question answering, machine translation, and text generation. Check out their \href{https://huggingface.co/models}{\textbf{model hub}} to browse pre-trained models.
    \item The library allows developers to quickly and easily integrate powerful NLP models into their applications using a simple API for loading pre-trained models. See the \href{https://huggingface.co/transformers/main_classes/pipelines.html}{\textbf{documentation}} for more details.
    \item The library includes a range of tools for fine-tuning models on custom datasets, making it easy to adapt models to specific tasks.
    \item Hugging Face has a large and active community that provides support, documentation, and a range of resources to help developers and researchers get the most out of the library. Join the community on their \href{https://discuss.huggingface.co/}{\textbf{forums}}.
    \item In addition to pre-trained models and tools, Hugging Face also provides datasets, evaluation metrics, and benchmarking tools for NLP. Explore their \href{https://huggingface.co/datasets}{\textbf{datasets}} and \href{https://huggingface.co/metrics}{\textbf{evaluation tools}} on their website.
    \item Hugging Face is a valuable resource for anyone working with NLP models, whether you are a developer looking to integrate models into your applications or a researcher looking to explore the state of the art in NLP. See how Hugging Face models have been used in various applications on their \href{https://huggingface.co/blog}{\textbf{blog}}.
\end{itemize}

\subsection{Benefits of using pre-trained models:}

\begin{itemize}
    \item Pre-trained models are already trained on vast amounts of data and have learned to perform well on a wide range of NLP tasks. This saves a lot of time and resources that would otherwise be spent on data collection, pre-processing, and model training.
    \item Hugging Face provides access to a large collection of pre-trained models for various NLP tasks, which are continually updated and improved. This allows developers and researchers to choose the best model for their specific use case and avoid the risk of building a suboptimal model from scratch.
    \item The Hugging Face Transformers library provides a simple API for loading and using pre-trained models, making it easy to integrate them into custom applications without requiring deep knowledge of NLP or machine learning.
\end{itemize}

\section{Benefits of Transformers}
\href{https://www.youtube.com/watch?v=lScR6pQPq9g}{Youtube} 
\subsection{Transformer Architecture Benefits}

\subsubsection{Faster to Train}
The replacement of recurrent cells with feedforward networks improves the parallelization of Transformers. Current high-performance computing systems are designed to work well with this type of parallelization.

\subsubsection{Better Performance}
Transformers offer better performance than RNNs across most natural language tasks. Therefore, we can use them to solve new problems.

\subsubsection{Versatility}
The Transformer architecture can move between different domains like NLP and Computer Vision.

\section{Intro to BERT}

\subsection{BERT Overview}

BERT (Bidirectional Encoder Representations from Transformers) is a Machine Learning (ML) model for natural language processing developed by Google in 2018. BERT is a versatile model that can handle a range of natural language processing (NLP) tasks, including but not limited to:

\begin{enumerate}
    \item Sentiment analysis
    \item Named entity recognition
    \item Question answering
    \item Language inference
    \item Text classification
    \item Paraphrasing
    \item Text summarization
    \item Machine translation
    \item Language modeling
    \item Text completion
    \item Entity linking
    \item Coreference resolution
\end{enumerate}
BERT's ability to perform well on these tasks makes it a valuable tool for many NLP applications.

\subsection{The Science Behind BERT: How it Learns and Processes Language}

To achieve its remarkable performance, BERT utilizes the following components:

\subsubsection{Extensive training data}
BERT was trained on a colossal dataset of 3.3 billion words, which is one of the main factors that contributed to its success. Specifically, it was trained on two vast datasets: Wikipedia (about 2.5 billion words) and Google's BooksCorpus (about 800 million words). By using these vast and varied datasets, BERT gained a deep understanding of natural language.

\subsubsection{MLM (Masked Language Modeling)}
MLM is a technique used by BERT to learn about the relationships between words in a sentence. In this process, BERT is trained to predict what a masked word should be based on the other words in the sentence.

\begin{quote}
\textit{Example:}

Let's say we have the following sentence: "\textit{The cat sat on the [MASK]}".

During pre-training, BERT may randomly mask one of the words in the sentence. In this case, let's say BERT masks the word "mat". The sentence would then look like this: "The cat sat on the [MASK]".

BERT is then trained to predict what the masked word should be based on the other words in the sentence. In this case, the correct answer is "mat". By considering the other words in the sentence, such as "cat" and "sat", BERT is able to make an educated guess that the missing word is "mat".

This process is repeated many times over with different sentences and different masked words, allowing BERT to learn about the relationships between words in a sentence and build a deep understanding of language.

\end{quote}

\subsubsection{NSP (Next Sentence Prediction)}
NSP is another technique used by BERT during pre-training to help it better understand the overall structure and flow of language. In this process, BERT is trained to predict whether two sentences are likely to appear together in a piece of text.

\begin{quote}
\textit{Example:}

Let's say we have two sentences:

\begin{itemize}
    \item "The cat sat on the mat."
    \item "It was a beautiful day outside."
\end{itemize}
During pre-training, BERT may be given these two sentences and asked to predict whether they are likely to appear together in a piece of text. In this case, the answer would be "no" since the two sentences do not seem to be related to each other.

BERT is trained using many different pairs of sentences, some of which are related and some of which are not. By learning to predict whether pairs of sentences are related or not, BERT gains a better understanding of the overall structure and flow of language.

This technique is important because it helps BERT understand the context in which sentences appear, which is crucial for many natural language processing tasks such as question answering and text classification.

\end{quote}

\subsection{BERT Architecture}
\includegraphics[width=0.5\linewidth]{bert.jpeg}
\captionof{figure}{BERTlarge - model architecture (\textit{340 million parameters})}

\begin{table}[!htbp]
\centering

\begin{tabular}{| l | l | l | l | l |}
\hline
Model & Transformer Layers & Hidden Size & Attention Heads & Parameters \\
\hline

\hline
BERTbase & 12 & 768 & 12 & 110M \\
\hline
BERTlarge & 24 & 1024 & 16 & 340M \\
\hline

\end{tabular}

\end{table}

The above table provides some key specifications of two different versions of the BERT model: \textbf{BERTbase} and \textbf{BERTlarge}.

\begin{itemize}
    \item \textbf{Transformer Layers:} This refers to the number of transformer layers in the BERT model. Transformer layers are a key component of BERT and are responsible for processing the input text.
    \item \textbf{Hidden Size:} This refers to the number of hidden units in each layer of the BERT model. This is an important parameter as it determines the capacity of the model to learn complex patterns in the input data.
    \item \textbf{Attention Heads:} This refers to the number of attention heads used in each transformer layer. Attention heads are responsible for computing the attention scores between different parts of the input sequence, which allows the model to focus on the most relevant parts of the input.
    \item \textbf{Parameters:} This refers to the total number of parameters in the BERT model. The number of parameters is directly proportional to the complexity of the model and determines how well it can fit the training data.
\end{itemize}

\input{rnn/text_classification_BERT}
\input{rnn/text_translation}