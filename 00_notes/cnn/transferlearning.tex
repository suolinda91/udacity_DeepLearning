\chapter{Transfer learning}
Modern deep neural networks are data-hungry. They require very large datasets, with millions of items, to reach their peak performance.\newline

Unfortunately, developing such large datasets from scratch for every use case of deep learning is very expensive and often not feasibe.\newline

Transfer learning is a technique that allows you take a neural network that has been already trained of one of these very large datasets, and tweak it slightly to adapt it to a new dataset.\newline

This requires far less data than training from scratch. This is the reason why transfer learning is a much more common technique for real-life applications than training from scratch.\newline

Using a pre-designed and pre-trained architecture instead of designing your own gives you most of the time the best results, and also saves a lot of time and experimentation.\newline

For all of these reasons, transfer learning is a technique of paramount importance for real-life uses of AI.

\section{Innovative CNN Architectures}

We will first take a look at some innovative CNN architectures that accomplished significant breakthroughs in the ImageNet competitions and had good success in real-world applications of AI.

\subsection{AlexNet, VGG and an Introduction to ResNet}
\href{https://www.youtube.com/watch?v=GdYOqihgb2k&ab_channel=Udacity}{Youtube}

\subsubsection{AlexNet}

The first CNN architecture to use the ReLU activation function. AlexNet also used DropOut to prevent overfitting. It has the structure of a classical CNN, with a backbone made of convolution and Max Pooling followed by a flattening and a Multi-Layer Perceptron. \newline

You can see the PyTorch code \href{https://github.com/pytorch/vision/blob/59c4de9123eb1d39bb700f7ae7780fb9c7217910/torchvision/models/alexnet.py\#L17}{\textbf{here}}.

\subsubsection{VGG}

This architecture was designed by the Visual Geometry Group at Oxford. There are two versions, VGG16 and VGG19, with 16 and 19 layers respectively. The designers pioneered the use of many 3 by 3 convolutions instead of fewer larger kernels (for example, the first layer of AlexNet uses a 11 by 11 convolution). Most CNNs up to today still use the same strategy. Apart from that, VGG has an elegant and regular architecture made of convolutional layers followed by Max Pooling layers. The height and width of the feature maps decreases as we go deeper into the network, thanks to the Max Pooling layers, but the number of feature maps increases. The backbone is then followed by a flattening operation and a regular head made of a Multi-Layer Perceptron.

\subsubsection{ResNet in Depth}
\href{https://www.youtube.com/watch?v=fPvp9WSXfK4&ab_channel=Udacity}{Youtube} \newline

ResNet is a very important architecture that introduced a fundamental innovation: the \textbf{skip connection}.\newline

Before ResNet, deep learning models could not go very deep in terms of number of layers. Indeed, after a certain point, going deeper was hurting performances instead of helping them.\newline

This pointed to problems in the optimization algorithm, because a deeper network should have at worst an identical performance to a shallower network. Indeed, the optimizer could transform the additional layers into the identity function and recover the shallower network exactly.\newline

The fact that this does not happen means that the optimizer has a hard time transforming the last layers in the identity function, and so it converges to a suboptimal solution that makes the second network WORSE than the first. This is largely due to the so-called \href{http://neuralnetworksanddeeplearning.com/chap5.html}{\textbf{vanishing gradient}} problem.\newline

How does ResNet solve this problem? By starting very close to the identity function, using the skip connection:

\includegraphics[width=0.5\linewidth]{img//cnn//transfer/skip-connection.jpeg}

In the ResNet block we have two convolutional layers with a ReLU in the middle. The output of these two layers is summed to the input tensor x and then another ReLU is applied on the result.\newline

This means that the central part comprising the two layers with the ReLU in the middle is learning the residual, from which comes the name Residual Network, or ResNet for short.\newline

It is easy to see how this block can become the identity function: it is sufficient to put the weights of the kernel of the first or the second convolutional layer to zero (or very close to zero). This will produce a feature map after the two convolutional layers where each pixel is zero. This is then summed to x, which means our block behaves as the identity function because H(x) = x.\newline

With this simple trick we can now go very deep (up to hundreds of layers) and increase significantly the performance of the network.\newline

We can implement the ResNet block in PyTorch as follows:
\begin{lstlisting}
class ResidualBlock(nn.Module):
    def __init__(self, inp, out1, out2):
        super().__init__()

        self.conv_block = nn.Sequential(
            nn.Conv2d(inp, out1, 3),
            nn.ReLU(),
            nn.Conv2d(out1, out2, 3)
        )
        self.relu = nn.ReLU()

    def forward(self, x):
        # F(x)
        F = self.conv_block(x)
        # IMPORTANT BIT: we sum the result of the
        # convolutions to the input image
        H = F + x
        # Now we apply ReLU and return
        return self.relu(H)
\end{lstlisting}

\subsubsection{Optional Resources to Explore Further}

\begin{itemize}
    \item Check out the \href{http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}{\textbf{AlexNet}} paper!
    \item Read more about \href{https://arxiv.org/pdf/1409.1556.pdf}{\textbf{VGGNet}} here.
    \item The \href{https://arxiv.org/pdf/1512.03385v1.pdf}{\textbf{ResNet}} paper can be found here.
    \item Read this \href{http://neuralnetworksanddeeplearning.com/chap5.html}{\textbf{detailed treatment}} of the vanishing gradients problem.
    \item Visit the \href{http://www.image-net.org/challenges/LSVRC/}{\textbf{ImageNet Large Scale Visual Recognition Competition (ILSVRC)}} website.
\end{itemize}

\section{Input Size and the GAP Layers}

\subsection{Fixed Input Size and Global Average Pooling (GAP)}

\subsubsection{Size of the Input Image for CNNs}

A classic CNN has a first section comprised of several layers of convolutions and pooling, followed by a flattening and then one or more fully-connected layers.

\includegraphics[width=1\linewidth]{img//cnn//transfer/cnn-structure-complete.jpeg}

Convolutional and pooling layers can handle any input size (they will just produce outputs of different size depending on the input size). However, fully-connected layers can only work with an input array of a specific size. Therefore, the vector produced by the flattening operation must have a specific number of elements, because it feeds into the fully-connected layers. \newline

Let's call this number of elements H. This means that the feature maps that are being flattened must have a specific size, so that n\_channels x height x width = H. Since the height and width of the last feature maps are determined by the size of the input image, as it flows through the convolutional and the pooling layers, this constraint on the vector produced by the flattening operation translates to a constraint on the size of the input image. Therefore, for CNNs using flattening layers, the input size must be decided a priori when designing the architecture.

\subsubsection{Global Average Pooling (GAP) Layer}

We can now introduce a new pooling layer that is widely used in modern CNNs. This type of pooling is equivalent to average pooling, but the average is taken over the entire feature map. It is equivalent to an Average Pooling Layer with the window size equal to the input size.\newline

This layer becomes very interesting because it can be used in place of the flattening operation at the end of the convolutional part of a CNN. Instead of taking the last feature maps and flattening them into a long vector, we take the average of each feature map and place them in a much shorter vector:

\includegraphics[width=1\linewidth]{img//cnn//transfer/dlnd-refresh-c2-cd1821-l2-2.jpeg}
\captionof{figure}{Flattening (left) vs Global Average Pooling (GAP, right)}

This drastically reduces the dimensionality of the resulting vector, from n\_channels x height x width to just n\_channels. But also, more importantly, it makes the network adaptable to any input size! Let's see how.\newline

If we use the GAP layer instead of flattening, we are going to obtain a vector of constant size independent of the size of the input image, because the size of the vector after the GAP layer is given by the number of feature maps in the last convolutional layer, and it is not influenced by their height and width. Therefore, the input image can have any size because this will not influence the number of feature maps, but only their height and width.\newline

Note however that a network with GAP trained on a certain image size will not respond well to drastically different image sizes, even though it will output a result. So effectively the input size became a tunable parameter that can be changed without affecting the architecture of the CNN.\newline

Many modern architectures adopt the GAP layer.

\section{Attention Layers}

\subsection{Attention}

The concept of attention is a very important concept in modern neural networks. It is a simple idea: the network should learn to boost some information that is helpful for a given example, and decrease the importance of information that is not useful for that example. \newline

There are several forms of attention. Let's look at two important ones.

\subsection{Channel Attention: Squeeze and Excitation}
\href{https://www.youtube.com/watch?v=ph43iEgPx6I&ab_channel=Udacity}{Youtube} \newline

The term "channel" can refer to the channels in the input image (3 channels if RGB) but also to the number of feature maps are output from a layer. \newline

Channel attention is a mechanism that a network can use to learn to pay more attention (i.e., to boost) feature maps that are useful for a specific example, and pay less attention to the others.\newline

This is accomplished by adding a sub-network that given the feature maps/channels assigns a scale to each input feature map. The feature maps with the largest scale are boosted.


\includegraphics[width=1\linewidth]{img//cnn//transfer/se_high_level_example.png}
\captionof{figure}{ }
\label{fig:se_high_level_example}
In \autoref{fig:se_high_level_example} we see how a specific type of channel attention works.
The squeeze and excitation block  (SE block) has 2 parts: 
\begin{enumerate}
    \item In the \textbf{squeeze part,} we apply GAP to this feature maps. Remember that the GAP  layer averages the content of each feature map. In this case, we will have a low average for the first two feature maps, because they are almost completely black. Instead a high average for the other two feature maps that contain a lot more information.
    \item Then there is an \textbf{excitation part}. This is just a multi-layer perceptron that takes as input the vector coming from the GAP layer, and based on the pattern in that vector, outputs an other vector of the same size as the number of feature maps. Each number in this vector will be multiplied by the corresponding feature map, to boost or to reduce the importance of that feature map. In this case, probably the perception will decide to boost the last two features that contain a lot of information, and decrease the importance of the first two. The numbers produced by the MLP are then multiplied by the feature maps as we just said, and the network can proceed.
\end{enumerate}

The most widely-used architectures with SE blocks (and many other useful tricks) are the EfficientNet and EfficientNetVS2 families.  \newline

Let's see now how these squeeze and excitation block can be implemented in PyTorch:

\includegraphics[width=0.25\linewidth]{img//cnn//transfer/se.jpeg}

\begin{lstlisting}
class SqueezeExcitation(nn.Module):
    def __init__(self, input_channels, squeeze_channels):
        super().__init__()
        self.squeeze = torch.nn.AdaptiveAvgPool2d(1) # GAP layer
        self.excitation = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_channels, squeeze_channels),
            nn.ReLU(),
            nn.Linear(squeeze_channels, input_channels),
            nn.Sigmoid() # squeeze the weights between 0 and 1
        )
    
    def forward(self, x):
    out = self.squeeze(x)
    scale = self.excitation(out).unsqueeze(-1).unsqueeze(-1)
    out = scale * x # Boost important feature maps

    return out
\end{lstlisting}
In the forward part of this network, we apply first the squeeze and then the excitation part. Then we multiply the scaling factors by the input to boost or reduce the importance of the input feature maps.
Then we return the result. \newline

We can use these squeezing excitation block in most architectures. For example, here you can see how to use it to modify a ResNet architecture. We just insert it between the residual part and the skip connections:


\includegraphics[width=0.25\linewidth]{img//cnn//transfer/se_in_resnet.png}
\captionof{figure}{Using the SE Block in a ResNet Architecture}
\label{fig:se_in_resnet}

\subsection{Brief Introduction to Transformers in Computer Vision}

Vision Transformers have been recently introduced, and are becoming more and more important for computer vision. They contain another form of attention, called \textbf{self attention}. \newline

\href{https://www.youtube.com/watch?v=E5HGkrHoL9I&ab_channel=Udacity}{Youtube} \newline

Note the vision transformers are a specific neural network architecture alternative to convolutional neural networks. Do not confuse them with the image transformations that are used to prepare the input to CNNs and to apply image augmentation. Presenting transformers in detail, will take an entire lesson on its own. We are just going to give you an introduction to them here, and develop your intuitive understanding of their function.

\subsubsection{Brief Notes on Vision Transformers}

Transformers are a family of neural networks originally developed for Natural Language Processing (NLP) applications. They are very good at modeling sequences, such as words in a sentence. They have been extended to deal with images by transforming images to sequences. In short, the image is divided in patches, the patches are transformed into embedded representations, and these representations are fed to a Transformer that treats them as a sequence. \newline

Transformers are characterized by the self-attention mechanism. Just like channel attention allows the network to learn to focus more on some channels, self attention allows the network to learn how to pay attention to the relationship between different words in a sentence or parts of an image. \newline

While CNNs build large Effective Receptive Fields by using many layers, vision Transformers show large receptive fields earlier and more consistently throughout the network.

\subsubsection{Additional Resources}

\begin{itemize}
    \item Introduction to transformers: \href{https://www.youtube.com/watch?v=dichIcUZfOw}{\textbf{video 1}}, \href{https://www.youtube.com/watch?v=mMa2PmYJlCo}{\textbf{video 2}}, \href{https://www.youtube.com/watch?v=gJ9kaJsE78k&t=2s}{\textbf{video 3}}
    \item \href{https://arxiv.org/abs/1706.03762}{\textbf{The paper that introduced transformers}}
    \item \href{https://arxiv.org/abs/2010.11929}{\textbf{The paper that introduced transformers for computer vision}}
\end{itemize}

\section{State of the Art Computer Vision Models}
\href{https://www.youtube.com/watch?v=omAM78zhf3Y&ab_channel=Udacity}{Youtube} \newline

Vision Transformers have state-of-the-art performances in many academic computer vision tasks. CNNs are, however, still by far the most widely-used models for real-world computer vision applications.

Transformers are very powerful but they need a lot more data than CNNs, and they are typically slower and more computationally expensive. CNNs are more data-efficient because they are built around two baked-in assumptions: local connectivity, which dictates that pixels close to each other are related (by using small kernels); and weight sharing, which dictates that different portions of an image must be processed identically (by sliding the same convolutional kernel across the entire image). Transformers are much more general, and they do not impose these assumptions. Therefore, they are more adaptable, but need more data to learn these characteristics of many images.

There are also architectures that are hybrids of CNNs and Transformers, which try to create the best combination of both, aiming to be data-efficient but more powerful than pure CNNs.

Summarizing, there are currently 3 categories of computer vision models:

\begin{itemize}
    \item Pure CNN architectures - still widely used for the majority of real-world applications. Examples: \href{https://arxiv.org/abs/2104.00298}{\textbf{EfficientNet V2}}, \href{https://arxiv.org/abs/2201.03545}{\textbf{ConvNeXt}}
    \item Pure Vision Transformers - currently widely used in academic environments and in large-scale real-world applications. Examples: \href{https://arxiv.org/abs/2010.11929}{\textbf{ViT}}, \href{https://arxiv.org/abs/2111.09883}{\textbf{Swin V2}}
    \item Hybrid architectures that mix elements of CNNs with elements of Transformers. Example: \href{https://arxiv.org/abs/2106.04803}{\textbf{CoatNet}}
\end{itemize}
As a final note, Transformers are now becoming even more important because they form the basis for multi-modal models - models that deal with, for example, image and text simultaneously. Examples of these are Open AI's \href{https://openai.com/blog/clip/}{\textbf{CLIP}} and Google's \href{https://imagen.research.google/}{\textbf{ImageGen}}.

\section{Quiz Question}

What is the role of the skip connection during a forward pass?
\begin{itemize}
    \item To skip part of the backbone and jump directly to the head
    \item \textbf{Makes it possible for a layer to easily learn the identity function, and forces the layer to learn the residuals with respect to the input image}
    \item To skip one or more channels
\end{itemize}

\section{Transfer Learning}
\href{https://www.youtube.com/watch?v=yfPEROi3SPU&ab_channel=Udacity}{Youtube} \newline

\textbf{Transfer learning} is a technique that allows us to re-use what a network has learned on one dataset to learn about a different dataset. \newline

While training from scratch requires large datasets and a lot of resources, transfer learning can be applied successfully on much smaller datasets without the need for large computational resources.

\section{Reusing Pre-Trained Networks}
\href{https://www.youtube.com/watch?v=kn4BN7z3UGQ&t=1s&ab_channel=Udacity}{Youtube} \newline

A normal CNN extracts more and more abstract features the deeper you go in the network. This means that the initial layers, which extract elementary features such as edges and colors, are probably pretty general and can be applied similarly on many different datasets. Instead, the last layers (especially the fully-connected layers) are highly specialized in the task they have been trained on. \newline

Accordingly, in transfer learning we keep the initial layers (that are pretty universal) unchanged or almost unchanged, while we change the last layers that must be specialized by task. \newline

How many layers we keep or modify slightly, and how many layers we change dramatically or even replace, depends on how similar our dataset is to the original dataset and on how much data we have.\newline

So essentially the transfer-learning workflow consists of taking a pre-trained model, freezing some of the initial layers and freeing or substituting some late layers, then training on our dataset.

\subsection{Getting a Pre-Trained Model with \lstinline{torchvision}}

You can find the list of all models supported by \lstinline|torchvision| in the \href{https://pytorch.org/vision/stable/models.html}{\textbf{official documentation}} (note that new models are added with each new version, so check that the list you are consulting is appropriate for the version of PyTorch you are using). Then you can load models by name. For example, let's load a \lstinline|resnet| architecture:
\begin{lstlisting}
import torchvision.models

model = torchvision.models.resnet18(pretrained=True)
\end{lstlisting}
The \lstinline{pretrained=True} option indicates that we want the weights obtained after training on ImageNet or some other dataset. If we set \lstinline{pretrained=False} we get the model initialized with the default initialization, ready to be trained from scratch.

\subsection{Freezing and Thawing Layers and Parameters}

A frozen parameter is a parameter that is not allowed to vary during training. In other words, backpropagation will ignore that parameter and won't change its value nor compute the gradient of the loss with respect to that parameter. \newline

In PyTorch you can freeze all the parameters of a network using the following code:
\begin{lstlisting}
for param in model.parameters():
    param.requires_grad = False
\end{lstlisting}
Similarly, you can also freeze the parameters of a single layer. For example, say that this layer is called \lstinline|fc|, then:
\begin{lstlisting}
for param in model.fc.parameters():
  param.requires_grad = False
\end{lstlisting}
You can instead thaw parameters that are frozen by setting \lstinline{requires_grad} to \lstinline|True|.

\subsubsection{BatchNorm}

The \lstinline|BatchNorm| layer is a special case: it has two parameters (gamma and beta), but it also has two buffers that are used to accumulate the mean and standard deviation of the dataset during training. If you only use \lstinline{requires_grad=False} then you are only fixing gamma and beta. The statistics about the dataset are still accumulated. Sometimes fixing those as well can help the performance, but not always. Experimentation, as usual, is key.\newline

If we want to also freeze the statistics accumulated we need to put the entire layer in evaluation mode by using \lstinline|eval| (instead of \lstinline{requires_grad=False} for its parameters):

\begin{lstlisting}
model.bn.eval()
\end{lstlisting}
Note that this is different than using \verb|model.eval()| (which would put the entire model in evaluation mode). You can invert this operation by putting the BatchNorm layer back into training mode: \verb|model.bn.train()|.

\subsection{Understanding the Architecture We Are Using}

When doing transfer learning, in many cases you need to know the layout of the architecture you are using so you can decide what to freeze or not to freeze. In particular, you often need either the name or the position of a specific layer in the network. \newline

As usual, we do not encourage the use of \verb|print(model)| as the output there does NOT necessarily correspond to the execution path coded in the \verb|forward| method. Instead, use the documentation of the model or export the model and visualize it with \href{https://netron.app/}{\textbf{Netron}} as explained in the next subsection. \newline

\subsubsection{Visualizing an Architecture with Netron}

Netron is a web app, so you do not need to install anything locally. First we need to export the model:
\begin{lstlisting}
# Fake image needed for torch.jit.trace
# (adjust the size of the image from 224x224 to what the
# network expects if needed)
random_image = torch.rand((1, 3, 224, 224))

scripted = torch.jit.trace(model, random_image)
torch.jit.save(scripted, "my_network.pt")
\end{lstlisting}
Then we can go to \href{https://netron.app/}{\textbf{Netron}} and load this file. Once the architecture has been loaded, press Crtl+U to visualize the name of each layer. For example, this is the last part of a ResNet architecture:

\includegraphics[width=1\linewidth]{img//cnn//transfer/resnet.jpeg}
\captionof{figure}{Last Layers of a ResNet Architecture}

The last layer is called \verb|fc|. Netron is also telling us that there are 1000 x 512 weights. This means that there are 512 inputs to the layer and 1000 outputs (the ImageNet classes are 1000). If we want to freeze the parameters of that layer we can do:

\begin{lstlisting}
for param in model.fc.parameters():
  param.requires_grad = False
\end{lstlisting}

\section{Fine Tuning}
\href{https://www.youtube.com/watch?v=XOyb315xYbw&ab_channel=Udacity}{Youtube} 

\subsection{Transfer Learning}

Transfer learning involves taking a pre-trained neural network trained on a source dataset (for example, Imagenet) and adapting it to a new, different dataset, typically a custom dataset for a specific problem.\newline

There are different types of transfer learning and different strategies that you can use depending on:

\begin{enumerate}
    \item The size of your dataset
    \item How similar your dataset is to the original dataset that the network was trained on (e.g., ImageNet)
\end{enumerate}
We can schematize the different possibilities like this:

\includegraphics[width=1\linewidth]{img//cnn//transfer/fine-tune-quadrants.jpeg}


\paragraph{Dataset Size}

It is difficult to define what a small dataset or a large dataset is exactly. However, for a typical classification example, a small dataset is in the range of 1000-10,000 images. A large dataset can have 100,000 images or more. These boundaries also change significantly depending on the size of the model you are using. A dataset that is large for a ResNet18 model (a ResNet with 18 layers) could be small for a ResNet150 architecture (a ResNet with 150 layers). The latter has many more parameters and a much larger capacity so it needs more data. In general, the larger the model, the more data it needs.

\paragraph{Dataset Similarity}

Similarly, it is sometimes difficult to judge whether a target dataset is similar to the source dataset. For example, if the source dataset is Imagenet and the target dataset is of natural images, then the two datasets are pretty similar. However, if the target is medical images then the datasets are fairly dissimilar. However, it must be noted that CNNs look at images differently than we do, so sometimes datasets that look different to us are sufficiently similar for the model, and vice-versa. It is important to experiment and verify our assumptions.

\subsubsection{Size of Dataset: What to Do}

\paragraph{Small target dataset, similar to the source dataset: Train the head}

This is a typical case, and the case where transfer learning really shines. We can use the pre-trained part of the network to extract meaningful feature vectors and use them to classify our images.\newline

In practice we take the head of the network and we substitute it with one or more new fully-connected layers (with the usual BatchNorm and ReLU layers in-between). Remember that the head of the network is the final part of the network, made typically by an MLP or similar, after the convolution part. It takes the output of the feature extractor part (also called \textit{backbone}) and uses it to determine the class of the image (in the case of image classification). In some architectures like ResNet the head is just one layer (the last layer); in other architectures it is more complicated, encompassing the last few layers. Of course, the last of these layers needs to have as many output nodes as classes in our problem (or one number in case of regression).

Then we train, keeping all the layers fixed except for the layer(s) we have just added.

For example, let's say we have 1000 images (a small dataset) and a classification task with 10 classes. This is what we could do:
\begin{lstlisting}
import torch.nn as nn
import torchvision.models

## Get a pre-trained model from torchvision, for example
## ResNet18
model = torchvision.models.resnet18(pretrained=True)

## Let's freeze all the parameters in the pre-trained
## network
for param in model.parameters():
    param.requires_grad = False

## Through Netron.app we have discovered that the last layer is called
## "fc" (for "fully-connected"). Let's find out how many input features
## it has
input_features = model.fc.in_features
## We have 10 classes
n_classes = 10
## Let's substitute the existing fully-connected last layer with our
## own (this will have all its parameters free to vary)
model.fc = nn.Linear(input_features, n_classes)
## or we can use a more complicated head (this might or might not
## lead to improved performances depending on the case)
model.fc = nn.Sequential(
    nn.BatchNorm1d(input_features),
    nn.Linear(input_features, input_features * 2),
    nn.ReLU(),
    nn.BatchNorm1d(input_features * 2),
    nn.Dropout(0.5),
    nn.Linear(input_features * 2, n_classes)
)
\end{lstlisting}
Now we can train our model as usual. You might want to start by executing the learning rate finder we have seen in our previous exercises and train for a few epochs. Depending on the size of your dataset, you might reach good performances rather quickly. It is likely that you will have to train for much less time than you would if you were to train from scratch. Be careful with overfitting and do not overtrain! If needed, also add more image augmentations, weight decay, and other regularization techniques.

\paragraph{Large dataset, at least somewhat similar to the original dataset: Fine-tune the entire network}

If we have more data and/or the task is not very similar to the task that the network was originally trained to solve, then we are going to get better performances by fine-tuning the entire network.\newline

We start by performing the same procedure as the previous case: we remove the existing head, we freeze everything and we add our own head, then we train for a few epochs. Typically 1 or 2 epochs are sufficient.\newline

We then free all the layers and we train until convergence (until the validation loss stops decreasing). We need to be very careful to use a conservative learning rate, to avoid destroying what the network has learned during the original task. A good choice is typically a value between 2 and 10 times smaller than the learning rate we used to train the head. As usual, experimentation is typically needed to find the best learning rate for this phase.\newline

A more advanced technique that works well in practice is to use a \href{https://arxiv.org/abs/1801.06146v5}{\textbf{different learning rate for every layer}}. You start with using the maximum learning rate for the last layer and you gradually decrease the learning rate for layers deeper into the network until you reach the minimum for the first convolutional layer.

\paragraph{Large dataset, very different than the original dataset: Train from scratch}

In this situation, fine-tuning does not give us better performance or faster training. We are better off just training from scratch. We can still take advantage of good architectures that performed well on ImageNet, since they are likely to work well on our dataset as well. We can just use them without pre-trained weights, for example:
\begin{lstlisting}
import torch.nn as nn
import torchvision.models

## Get a pre-trained model from torchvision, for example
## ResNet18
model = torchvision.models.resnet18(pretrained=False)
\end{lstlisting}
\paragraph{Small dataset, very different than the original dataset: Gather more data or use semi-supervised learning}

This is the hardest situation. If you have tried fine-tuning just the head and it did not perform well enough, and fine-tuning more layers resulted in overfitting, you probably need to either collect more data or look into starting from scratch and use \href{https://en.wikipedia.org/wiki/Semi-supervised_learning}{\textbf{semi-supervised learning}}.

\paragraph{Other situations}

When it is not clear whether you are in any of the situations described above, you can take approaches that are in-between.\newline

For example, when you have a dataset that is not very small but not very large either, you might get good performances by fine-tuning not only the head, but also a few of the last convolutional layers or blocks. Indeed, these layers encode high-level concepts such as "squares," "triangles," or textures, and therefore can improve by being fine-tuned or even trained from scratch on your data. Just free those layers along with the new head and train those, while keeping the rest fixed. Depending once again on the size of your data and the similarity with the original dataset, you can fine-tune these layers or reset them and train them from scratch. As usual, it takes a bit of experimentation to find the best solution.

\subsection{TIMM: A Very Useful Library for Fine-Tuning}

When performing fine-tuning (or training with a predefined architecture), we cannot avoid mentioning the fantastic \href{https://github.com/rwightman/pytorch-image-models}{\textbf{PyTorch Image Models (timm) library}}. It contains hundreds of models, many with pre-trained weights, and it keeps getting updated with the very latest architectures from the research community. It is very easy to use it for transfer learning. It is an alternative to \verb|torchvision| that contains many more pretrained models.

First let's install it with:
\begin{lstlisting}
pip install timm
\end{lstlisting}
Then we can get a pre-trained model with a custom head just by doing:
\begin{lstlisting}
n_classes = 196
model = timm.create_model("convnext_small", pretrained=True, num_classes=n_classes)
\end{lstlisting}
The library already builds a head for you, so you do not need to build one explicitly. \newline

We can now choose to freeze some or all the layers except the last one, depending on the size of our dataset and its characteristics, and apply the techniques we discussed before. \newline

Note that you do not need to know the details of the architecture to be able to make a new head for it, as timm does that for you.
\section{Exercise Solution: Transfer Learning, Flowers}
\href{https://www.youtube.com/watch?v=SrJ9--3lsnQ&ab_channel=Udacity}{Youtube} 

\section{Visualizing CNNs (part 1)}

In this lesson we have looked at how to adapt pre-trained networks to our custom datasets. In a previous lesson you have learned how to train CNNs from scratch. In both cases, the final product is a trained network saved on disk. Now we are going to briefly see how to open that black box and look inside, to see what a trained network has learned and how the information travels through the network and gets processed. \newline

\href{https://www.youtube.com/watch?v=IVnPArwqsFU&ab_channel=Udacity}{Youtube} \newline

You can play on your own with the CNN Explainer website \href{https://poloclub.github.io/cnn-explainer/}{\textbf{here}} and verify that what you see there matches your expectations, given what you now know about CNNs.

\subsection{Optional Resources}

If you would like to know more about interpreting CNNs and convolutional layers in particular, you are encouraged to check out these resources:

\paragraph{What is the network looking at?}

\href{https://github.com/jacobgil/pytorch-grad-cam}{\textbf{This library}} implements several methods to interpret the decision of a CNN when classifying an image. It also contains references to the relevant papers.

\paragraph{Visualizing CNN layers}

\begin{itemize}
    \item Here's a \href{http://cs231n.github.io/understanding-cnn/}{\textbf{section}} from the Stanford CS231n course on visualizing what CNNs learn.
    \item Here's a \href{https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s}{\textbf{demonstration}} of another visualization tool for CNNs. If you'd like to learn more about how these visualizations are made, check out this \href{https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s}{\textbf{video}}.
    \item Read this \href{https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html}{\textbf{Keras blog post}} on visualizing how CNNs see the world. In this post, you can find an accessible introduction to Deep Dreams. When you've read that:

\begin{itemize}
        \item Also check out this \href{https://www.youtube.com/watch?v=XatXy6ZhKZw}{\textbf{music video}} that makes use of Deep Dreams (look at 3:15-3:40)!
        \item Create your own Deep Dreams (without writing any code!) using this \href{https://deepdreamgenerator.com/}{\textbf{website}}.
\end{itemize}

    \item If you'd like to read more about interpretability of CNNs, here's an \href{https://openai.com/research/attacking-machine-learning-with-adversarial-examples}{\textbf{article}} that details some dangers from using deep learning models (that are not fully interpretable) in real-world applications.
\end{itemize}

\section{Visualizing CNNs (part 2)}

\subsection{Visualizing CNNs}

Let’s look at a sample CNN that has been pre-trained on ImageNet to see how it works in action. This is an example of an architecture for a transfer learning use case. \newline

Let's verify that what we have discussed in this lesson matches what we see, in particular the fact that the first layers focus on edges and low-level features and can therefore be recycled even on different datasets. \newline

The CNN we will look at is trained on ImageNet as described in \href{https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf}{\textbf{this paper}} by Zeiler and Fergus. In the images below (from the same paper), we’ll see \textit{what} each layer in this network detects and see \textit{how} each layer detects more and more complex ideas.

\includegraphics{img//cnn//transfer/layer-1-grid.png}
\captionof{figure}{Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).}

The images above are from Matthew Zeiler and Rob Fergus' \href{https://www.youtube.com/watch?v=ghEmQSxT6tw}{\textbf{deep visualization toolbox}}, which lets us visualize what each layer in a CNN focuses on.\newline

Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.

\includegraphics{img//cnn//transfer/diagonal-line-1.png}
\captionof{figure}{As visualized here, the first layer of the CNN can recognize -45 degree lines.}

\includegraphics{img//cnn//transfer/diagonal-line-2.png}
\captionof{figure}{The first layer of the CNN is also able to recognize +45 degree lines, like the one above.}

Let's now see some examples of images that cause such activations. The grid of images below all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.

\includegraphics{img//cnn//transfer/grid-layer-1.png}
\captionof{figure}{Sample patches that activate the -45 degree line detector in the first layer}

So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs.

\subsubsection{Layer 2}

\includegraphics[width=1\linewidth]{img//cnn//transfer/screen-shot-2016-11-24-at-12.09.02-pm.png}
\captionof{figure}{A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or "what it sees") based on the corresponding images from the grid on the right.}

The second layer of the CNN captures complex ideas.

As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).

\textbf{The CNN learns to do this on its own.} There is no special instruction for the CNN to focus on more complex objects in deeper layers. That's just how it normally works out when you feed training data into a CNN.

\subsubsection{Layer 3}

\includegraphics[width=1\linewidth]{img//cnn//transfer/screen-shot-2016-11-24-at-12.09.24-pm.png}
\captionof{figure}{A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or "what it sees") based on the corresponding images from the grid on the right.}

The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column).

We'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.

\subsubsection{Layer 5}

\includegraphics[width=1\linewidth]{img//cnn//transfer/screen-shot-2016-11-24-at-12.08.11-pm.png}
\captionof{figure}{A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or "what it sees") based on the corresponding images from the grid on the right.}

The fifth (last) layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles.

\section{Glossary}

For your reference, here are all the new terms we introduced in this lesson: \newline

\textbf{Skip connection:} An innovation of ResNet, this is a path in a network allowing it to jump a layer if needed. \newline

\textbf{Global Average Pooling (GAP) layer:} A type of pooling equivalent to Average Pooling, but with the average taken over the entire feature map. It is equivalent to an Average Pooling layer with the window size equal to the input size. \newline

\textbf{Channel Attention (Squeeze-and-excitation, or SE, block)}: A little network-in-network that allows the model to pay more attention to some feature maps that are more useful than others to classify a specific image. \newline

\textbf{Self Attention}: A mechanism alternative to convolution+pooling and characteristic of the Transformers architecture. It allows the model to directly learn the relationship between different parts of the image and use it to solve the task at hand (e.g., image classification). \newline

\textbf{Transfer learning}: A set of techniques that allow to re-use what a network has learned from a dataset on a different dataset. It allows us to get very good performances much more quickly, and on smaller datasets, than training from scratch. \newline

\textbf{Frozen parameter}: A parameter that is not allowed to vary during training.
