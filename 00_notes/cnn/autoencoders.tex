\chapter{Autoencoders}

\section{Introduction to Autoencoders}
\href{https://www.youtube.com/watch?v=wxxsggRB4RE&ab_channel=Udacity}{Youtube} \newline

\textbf{Autoencoders} are a very interesting neural network architecture that can be used for different applications directly (anomaly detection, denoising...), and it is also widely used in larger and modern architectures for other tasks (object detection, image segmentation). \newline

More advanced versions of autoencoders, such as variational autoencoders, can also be used as generative models, i.e., they can learn representations of data and use that representation to generate new realistic images.

\section{Autoencoders}
\href{https://www.youtube.com/watch?v=a5zHMWOq0fc&ab_channel=Udacity}{Youtube} \newline

When studying CNNs for image classification or regression we have seen that the network is essentially composed of two parts: a backbone that extracts features from the image, and a Multi-Layer Perceptron or similar that uses those features to decide which class the image belongs to. In-between the two we have a flattening operation (or a Global Average Pooling layer) that takes the last feature maps coming out of the backbone and transforms them into a 1d array, which is a feature vector.

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/cnn-structure-recap.jpeg}
\captionof{figure}{Structure of a Typical CNN}

Autoencoders have a similar backbone (called \textbf{encoder} in this context) that produces a feature vector (called \textbf{embedding} in this context). However, they substitute the fully-connected layers (the \textbf{head}) with a \textbf{decoder} stage whose scope is to reconstruct the input image starting from the embeddings:

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/encoder-decoder.jpeg}
\captionof{figure}{Typical Structure of an Autoencoder}

This can appear pointless at first glance, but it is actually very useful in many contexts.

\subsection{Uses of Autoencoders}

We can use autoencoders to:

\begin{itemize}
    \item Compress data
    \item \textbf{Denoise} data
    \item Find outliers (do \textbf{anomaly detection}) in a dataset
    \item \textbf{Do inpainting} (i.e., reconstruct missing areas of an image or a vector)
    \item With some modifications, we can use autoencoders as \textbf{generative models} - models capable of generating new images
\end{itemize}
Autoencoders are also the basis for a whole field of research concerned with \textbf{metric learning}, which is learning representations of images that can be useful in downstream tasks.

\subsection{Unsupervised vs. Supervised Learning}

By looking closer at the structure and the tasks we have just described, you can see that autoencoders do not use the information on the label of the image at all. They are only concerned with the image itself, not with its label. The tasks that autoencoders address are examples of \textit{unsupervised learning}, where the algorithm can learn from a dataset without any label. Another example of unsupervised learning that you might be familiar with is \textit{clustering}. \newline

CNNs for image classification are instead an example of \textit{supervised learning}, where the network learns to distinguish between classes by learning from a labeled dataset.

\subsection{The Loss of Autoencoders}

The autoencoder is concerned with encoding the input to a compressed representation, and then re-constructing the original image from the compressed representation. The signal to train the network comes from the differences between the input and the output of the autoencoder. \newline

For example, let's consider an autoencoder for images. We compare the input image to the output image and we want them to be as similar as possible. \newline

What is the right loss for this task? \newline

We have different possibilities, but the most common one is the Mean Squared Error (MSE) loss. It just considers the square of the difference between each pixel in the input image and the corresponding pixel in the output image, so minimizing this loss is equivalent to minimizing the difference of each pixel in the input with the corresponding pixel in the output. In practice, this is given by the formula: 

\[MSE\ = \frac{1}{n_{rows} n_{cols}} \sum_{i=1}^{n_{rows}} \sum_{j=1}^{n_{cols}} (x_{ij} - \hat{x}_{ij}^2)\]

We take the square of the difference between each pixel \(x_{ij}\) in the input and the corresponding pixel \(\hat{x}_{ij}\) in the output and we average them out, i.e., we sum all the squared differences and then we divide by the number of pixels (which is equal to the number of rows \(n_{rows}\) times the number of columns \(n_{cols}\)). \newline

This loss is very simple and in practice gives good results. Using other types of losses is possible as well.

\section{A Linear Autoencoder}
\href{https://www.youtube.com/watch?v=Wo6TRsHNJEY&t=2s&ab_channel=Udacity}{Youtube} \newline

You will get a chance in the exercise upcoming to try out this code on your own! \newline

In this first look we have built the simplest autoencoder, which is made up of two linear layers:
\begin{lstlisting}
class Autoencoder(nn.Module):

    def __init__(self, encoding_dim):
        super(Autoencoder, self).__init__()
        ## encoder ##
        self.encoder = nn.Sequential(
            nn.Linear(28*28, encoding_dim),
            nn.ReLU(),
            nn.BatchNorm1d(encoding_dim)
        )

        ## decoder ##
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 28*28),
            nn.Sigmoid()
        )

        self.auto_encoder = nn.Sequential(
            nn.Flatten(),
            self.encoder,
            self.decoder
        )

    def forward(self, x):
        # define feedforward behavior 
        # and scale the *output* layer with a sigmoid activation function

        encoded = self.auto_encoder(x)

        # Reshape the output as an image
        # remember that the shape should be (batch_size, channel_count, height, width)
        return encoded.reshape((x.shape[0], 1, 28, 28))
\end{lstlisting}
We have trained it using the Mean Squared Error (MSE) loss. Of course, we did not use the labels, since anomaly detection with autoencoders is an unsupervised task.

\section{Quiz: Autoencoders}

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/image.png}

\section{Exercise Solution: Linear Autoencoder}
\href{https://www.youtube.com/watch?v=VVdYTc2qxV0&ab_channel=Udacity}{Youtube} \newline

This is the autoencoder network that I used in the solution to the exercise, for your reference:
\begin{lstlisting}
class Autoencoder(nn.Module):

    def __init__(self, encoding_dim):
        super(Autoencoder, self).__init__()
        ## encoder ##
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Linear(256, encoding_dim),
            nn.ReLU(),
            nn.BatchNorm1d(encoding_dim),

        )

        ## decoder ##
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Linear(256, 28*28),
            nn.Sigmoid()
        )

        self.auto_encoder = nn.Sequential(
            nn.Flatten(),
            self.encoder,
            self.decoder
        )

    def forward(self, x):
        # define feedforward behavior 
        # and scale the *output* layer with a sigmoid activation function

        encoded = self.auto_encoder(x)

        # Reshape the output as an image
        # remember that the shape should be (batch_size, channel_count, height, width)
        return encoded.reshape((x.shape[0], 1, 28, 28))
\end{lstlisting}

\subsection{Autoencoders for anomaly detection}

Now that you tried out using autoencoders for anomaly detection, try to explain how exactly these algorithms can pinpoint examples that are somewhat dissimilar from the rest of the dataset.\newline

This is an open-ended question, so your answer will be different from oursâ€”but after you submit your response, we'll show you how we thought about the problem and point out some key things to consider.

\subsubsection{Things to think about}

Autoencoders compress the visual information contained in images into a compact, latent representation (the embedding) that has a much lower dimensionality than the input image. By asking the decoder to reconstruct the input from this compact representation, we force the network to learn an embedding that stores meaningful information about the content of the image. For example, in the solution I compressed 28 x 28 images (so 784 pixels) into a vector of only 32 elements, but I was still able to reconstruct most of the images very well. \newline

When applying it to a test set that the network has never seen, most images were reconstructed well, but some of them were not. This means that the compression that the network has learned on the training dataset works well for the vast majority of the examples in this new set, but not for these anomalous ones. These anomalies have characteristics that the network is not well equipped to reconstruct, and therefore the decoder cannot recreate them faithfully during decoding. \newline

Through scoring each example by the loss, we are able to identify anomalies by simply taking the examples with the highest loss.

\section{Learnable Upsampling}
\href{https://www.youtube.com/watch?v=KjztLwPksj8&ab_channel=Udacity}{Youtube} \newline

We have seen how to use linear layers to create an autoencoder. Since we are working on images, it is natural to use convolution instead of just linear layers. Convolution allows us to keep spatial information and get a much more powerful representation of the content of an image. \newline

However, this poses a problem: while the encoder section can be just the backbone of a standard CNN, what about the decoder part? Yes, we could flatten the output of the backbone and then use linear layers to decode. But there are also other ways to upsample a compact representation into a full-resolution image. For example, we can use a Transposed Convolutional Layer, which can learn how to best upsample an image. \newline


You want to reverse the downsampling process that happened in the encoder. You want to increase the spatial dimensions of a compressed input to produce a reconstructed image that has the same shape as the original input. So, instead of down sampling using maxpooling, you can imagine trying to upsample an image by un-pooling the pixel values in an input.
You could use an interpolation technique like \textbf{nearest neighbors} or another kind of linear interpolation.

\includegraphics[width=0.5\linewidth]{img//cnn//autoencoders/upsamplingNN.png}

Nearest neighbors expands a given area by copying over a single pixel value from an input image to a \(2 \times 2\) two in the larger output image. If you've ever tried to enlarge a low resolution image this upsampling is usually what happens. \newline

But interpolation is a fairly crude way to upsample an image. In the case of nearest neighbors, we are just copying over existing values. But a realistic larger image is likely to have more variety in pixel values, and so there may be a better way to upsample its image. So, you could also try to learn how to best upsample an image. If we want our network to learn how to upsample, we can use a \textbf{Transposed Convolutional Layer}. This layer does not use a predefined interpolation method, instead it has learnable parameters. You can think of transpose convolution as a way to upsample existing input values using filter weights, in a way that is similar to traditional convolution.


\section{Transposed Convolutions}
\href{https://www.youtube.com/watch?v=hnnLAC1Q0zg&t=3s&ab_channel=Udacity}{Youtube} \newline

The Transposed Convolution can perform an upsampling of the input with learned weights. In particular, a Transposed Convolution with a 2 x 2 filter and a stride of 2 will double the size of the input image. \newline

Whereas a Max Pooling operation with a 2 x 2 window and a stride of 2 reduces the input size by half, a Transposed Convolution with a 2 x 2 filter and a stride of 2 will double the input size. \newline

Let's consider an autoencoder with two Max Pooling layers in the encoder, both having a 2 x 2 window and a stride of 2. If we want the network to output an image with the same size as the input, we need to counteract the two Max Pooling layers in the encoder with two Transposed Convolution layers with a 2 x 2 filter and a stride of 2 in the decoder. This will give us back an output with the same size as the input.

\subsection{Transposed Convolutions in PyTorch}

You can generate a Transposed Convolution Layer in PyTorch with:
\begin{lstlisting}
unpool = nn.ConvTranspose2d(input_ch, output_ch, kernel_size, stride=2)
\end{lstlisting}
See \href{https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html}{\textbf{here}} for the full documentation. \newline

For example, we can generate a Transposed Convolution Layer that doubles the size of an input grayscale image and generates 16 feature maps as follows:
\begin{lstlisting}
unpool = nn.ConvTranspose2d(1, 16, 2, stride=2)
\end{lstlisting}

\subsection{Alternative to a Transposed Convolution}

It \href{https://distill.pub/2016/deconv-checkerboard/}{\textbf{can be shown}} that Transposed Convolutions tend to produce checkerboard artifacts in the output of the networks. Therefore, nowadays many practitioners replace them with a nearest-neighbor upsampling operation followed by a convolution operation. The convolution makes the image produced by the nearest-neighbors smoother. For example, we can replace this Transposed Convolution:
\begin{lstlisting}
unpool = nn.ConvTranspose2d(1, 16, 2, stride=2)
\end{lstlisting}
with:
\begin{lstlisting}
unpool = nn.Sequential(
    nn.Upsample(scale_factor = 2, mode='nearest'),
    nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
)
\end{lstlisting}

\section{Convolutional Autoencoder}
\href{https://www.youtube.com/watch?v=bwIWNLEJYUw&ab_channel=Udacity}{Youtube} \newline

The simplest autoencoder using CNNs can be constructed with a convolutional layer followed by Max Pooling, and then an unpooling operation (such as a Transposed Convolution) that brings the image back to its original size:
\begin{lstlisting}
class Autoencoder(nn.Module):
    def __init__(self, encoding_dim):
        super(Autoencoder, self).__init__()

        ## encoder ##
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 3, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )

        ## decoder ##
        self.decoder = nn.Sequential(
            # Undo the Max Pooling
            nn.ConvTranspose2d(3, 1, 2, stride=2),
            nn.Sigmoid()
        )

        self.auto_encoder = nn.Sequential(
            self.encoder,
            self.decoder
        )

    def forward(self, x):
        # define feedforward behavior 
        # and scale the *output* layer with a sigmoid activation function

        return self.auto_encoder(x)
\end{lstlisting}
Of course, this autoencoder is not very performant. Typically you want to compress the information much more with a deeper encoder, and then uncompress it with a deeper decoder. This is what you are going to do in the next exercise. \newline

In real-life situations, you can also use an already-existing architecture like a ResNet to extract the features (just remember to remove the final linear layers, i.e., the \textit{head} and only keep the \textit{backbone}). Of course, your \textit{decoder} needs to then start from the embedding built by the architecture to get back to the dimension of the input image.

\section{Quiz Question}

Below are the some of the concepts we just discussed. Can you match each of them with the correct description?

\includegraphics[width=0.75\linewidth]{img//cnn//autoencoders/quiz1.png}


\section{Quiz Question}

Have a look at this encoder:
\begin{lstlisting}
encoder = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
 )
\end{lstlisting}
How many Transposed Convolutional layers with a kernel size of 2 and a stride of 2 do we need to upsample the result of the encoder back to the input shape?
\begin{itemize}
    \item One
    \item \textbf{Two}
    \item Three
    \item Four
\end{itemize}
Solution: The Transposed Convolution can be thought as the "inverse" of a 2 x 2 Max Pooling operation, in the sense that while \verb|MaxPool2d(2, 2)| halves the size of the input, a Transposed Convolution with a kernel size of 2 and a stride of 2 doubles the input size. So if there are 2 Max Pooling operations, we need two Transposed Convolution operations.

\section{Quiz Question}

Consider the following scenario:

You are training an autoencoder on RGB images that are 256 pixels high and 256 pixels wide. Your autoencoder has the following architecture for the encoder part:
\begin{lstlisting}
encoder = nn.Sequential(
        nn.Conv2d(3, 32, 3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(32, 32, 3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
 )
\end{lstlisting}
What is the shape of the embedding, i.e., of the vector computed with \verb|encoder(x)|, if \verb|x| is a batch of 64 input images?

\begin{itemize}
    \item (64,3, 256, 256)
    \item (64,1,28, 28)
    \item \textbf{(64, 32, 64,64)}
    \item (64,3, 128, 128)
\end{itemize}
Solution: We have two Max Pooling layers, and the convolutional layers are using a kernel size of 3 and a padding of 1 so they do not affect the shape. If we start with a shape of (64, 3, 256, 256), corresponding to 64 images with 3 channels and a height and a width of 256, then we halve the image size after each Max Pooling layer. So 256 / 2 / 2 = 64.


\section{Exercise Solution: Convolutional Autoencoder}
\href{https://www.youtube.com/watch?v=GE7FMPzzBSs&t=8s&ab_channel=Udacity}{Youtube}


\section{Denoising}

We call \textbf{denoising} the task of removing noise from an image by reconstructing a denoised image.\newline

This is a task that convolutional autoencoders are well-suited for.

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/autoencoder-denoise.jpeg}
\captionof{figure}{A Denoising Autoencoder}

A denoising autoencoder is a normal autoencoder, but trained in a specific way.

\subsection{How Do We Train a Denoising Autoencoder?}

In order to train a denoising autoencoder we need to have access to the denoised version of the images. The easiest way to do this is to build a training dataset by taking clean images and adding noise to them. Then we will feed the image with the added noise into the autoencoder, and ask it to reconstruct the denoised (original) version. \newline

It is very important that we then compute the loss by comparing the input \textit{uncorrupted} image (without noise) and the output of the network. \textit{DO NOT} use the noisy version when computing the loss, otherwise your network will not learn!

\subsection{Why Does it Work?}

Let's consider an autoencoder trained on a noisy version of the MNIST dataset. During training, the autoencoder sees many examples of all the numbers. Each number has noisy pixels in different places. Hence, even though each number is corrupted by noise, the autoencoder can piece together a good representation for each number by learning different pieces from different examples. Here the convolutional structure helps a lot, because after a few layers the convolution smooths out a lot of the noise in a blurry but useful image of the number. This is also why generally you need to go quite deep with CNN autoencoders if you want to use them for denoising.

\subsection{Quiz Question}

If the tensor \verb|images| represents a batch of uncorrupted images, and \verb|noisy_images| represents a batch of images where we added noise, which is the correct application of the loss?

(Assume that \verb|outputs| is a batch of outputs from the auto-encoder, and \verb|loss = nn.MSELoss()|.)
\begin{itemize}
    \item loss(images, outputs)
    \item loss(noisy\_images, outputs)
    \item loss(images, noisy\_images)
    \item loss(outputs, labels)
\end{itemize}

\section{Exercise Solution: Denoising Autoencoders}
\href{https://www.youtube.com/watch?v=Q2Af3dl-328&ab_channel=Udacity}{Youtube}


\section{Autoencoders and Generative Models}

As we have seen, an autoencoder has an \textit{encoder} part that compresses the input into a vector (\textit{embedding}) and a \textit{decoder} part that takes the embedding and tries to regenerate the input.\newline

Let's look closer at the embeddings for the MNIST dataset. Even though we did not use the labels for training the autoencoder, we can use them for visualization purposes and see if the embedding that the encoder has learned separates well the various classes. After all, if the encoder has learned the latent characteristics that distinguish a 3 from a 1 or a 8 from a 7, then the embedding space should reflect this structure.\newline

Let's consider the CNN autoencoder we presented as a solution to the Convolutional Encoder exercise of this lesson. It has a latent space (where the embeddings live) of 32 feature maps each 7 x 7 pixels. This corresponds to 1568 numbers. Of course we cannot visualize a space with 1568 dimensions, so we are going to use the \href{https://arxiv.org/abs/1802.03426}{\textbf{UMAP}} dimensionality reduction technique to visualize it in 2d:

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/mnist-umap.jpeg}

Here the different colors correspond to the different classes in the MNIST dataset (the different digits). To make things easier, I annotated each cluster with the label of the most common class in the cluster. \newline

It is indeed clear that images representing similar numbers are clustered together. Not only are most of the points belonging to the same class close to each other, but also numbers that are visually similar to each other (like 3, 8 and 5) are close in the latent space. \newline

Looking at this we could ask: what happens if we \textit{generate} an embedding vector close to one of these clusters, and run it through the decoder? We should be able to generate new numbers! This is indeed the case. If we take a few points in the embedding space and run them through the decoder, we obtain images such as:

\includegraphics[width=1\linewidth]{img//cnn//autoencoders/generated-mnist.jpeg}

We have just generated new MNIST images! \newline

However, if we repeat this exercise enough we soon realize that things don't look so great. The embedding space of an autoencoder is discontinuous: you can take an embedding of a 3, modify it just a tiny bit and end up with a completely different number, or even something that does not resemble a number at all. Why? Because in our training we use a loss that does not enforce any particular structure in the embedding space, so the network finds the one that happens to solve our problem best, without considering any constraints regarding the structure of the space. \newline

In more formal terms we can say that the autoencoder learns a mapping between our images and the embedding space. It does not learn the \textit{distribution} of the data in the embedding space. \newline

This problem can be solved by other algorithms, for example the so-called \textbf{Variational Autoencoders (VAEs)}. They learn to project our points in an embedding space that has much more structure than a simple autoencoder. VAEs are proper generative models, in that they learn to represent the distribution of the dataset and therefore their embedding space is much more regular and more suited for data generation. \newline

A deeper dive into VAEs goes beyond the scope of this class, but you can find more information \href{https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed}{\textbf{here}}. With what you learned in this lesson you should be able to learn VAEs in a breeze!

\section{Glossary}

For your reference, here are all the new terms we introduced in this lesson:

\textbf{Autoencoder}: A neural network architecture consisting of an encoder part, which takes an input and compresses it into a low-dimensional embedding vector, and a decoder part, which takes the embedding and tries to reconstruct the input image from it.

\textbf{Transposed Convolution}: A special type of convolution that can be used to intelligently upsample an image or a feature map

\textbf{Denoising}: The task of taking an image corrupted by noise and generating a version of the image where the noise has been removed.

\textbf{Variational autoencoder (VAE)}: An extension of the idea of autoencoders that transforms them into proper generative models.