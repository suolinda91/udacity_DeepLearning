{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Convolutional Autoencoder for Anomaly Detection\n",
    "\n",
    "Anomaly detection is the task of finding anomalous data elements in a dataset. An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
    "\n",
    "We are going to train an autoencoder on the MNIST dataset (that only contains numbers), and then we will look into anomalies within the MNIST dataset (i.e., images within MNIST that are somehow different than the rest of the dataset).\n",
    "\n",
    "Even though MNIST is a labeled dataset, we are going to disregard the labels for educational purposes and consider it as an unlabeled datasets.\n",
    "\n",
    "**This time we are going to use a CNN-based autoencoder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt | grep -v \"already\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After installing the dependencies you need to restart your kernel. The following cell does that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from helpers import get_data_loaders\n",
    "from helpers import seed_all\n",
    "from helpers import anomaly_detection_display\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure repeatibility\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get data loaders for the MNIST dataset for the train, validation\n",
    "# and test dataset\n",
    "data_loaders = get_data_loaders(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(data_loaders['train'])\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig, sub = plt.subplots(figsize = (2,2)) \n",
    "sub.imshow(img, cmap='gray')\n",
    "_ = sub.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Convolutional Autoencoder\n",
    "\n",
    "Write your own CNN autoencoder. Use at least 2 blocks Convolution + ReLU + MaxPooling as _encoder_, and then an equivalent number of upsampling operations (either Transposed Convolutions+ReLU or Upsample+Conv+ReLU) followed by a Sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        ## encoder ##\n",
    "        self.encoder = # YOUR CODE HERE\n",
    "        \n",
    "        ## decoder ##\n",
    "        self.decoder = # YOUR CODE HERE\n",
    "        \n",
    "        self.auto_encoder = nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        \n",
    "        return self.auto_encoder(x)\n",
    "    \n",
    "# initialize the NN\n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Set up here a loss function that makes sense for the task at hand (look at the lesson again if you don't remember what this should be):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to a normal training loop - however, this task is an unsupervised task. That means we do not need labels. The MNIST dataset does provide labels, of course, so we will just disregard them.\n",
    "\n",
    "Complete the training loop below. As usual, you need to perform the forward and the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in tqdm(desc=\"Training\", total=len(data_loaders['train']), iterable=data_loaders['train']):\n",
    "        # we disregard the labels. We use the Python convention of calling\n",
    "        # an unused variable \"_\"\n",
    "        images, _ = data\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(desc=\"Validating\", total=len(data_loaders['valid']), iterable=data_loaders['valid']):\n",
    "            # _ stands in for labels, here\n",
    "            images, _ = data\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs.flatten(), images.flatten())\n",
    "            \n",
    "            # update running training loss\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # print avg training statistics\n",
    "    train_loss /= len(data_loaders['train'])\n",
    "    val_loss /= len(data_loaders['valid'])\n",
    "    print(\"Epoch: {} \\tTraining Loss: {:.6f}\\tValid Loss: {:.6f}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 50 \tTraining Loss: 1.359510\tValid Loss: 1.399772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Anomalies\n",
    "Now that our autoencoder is trained we can use it to find anomalies. Let's consider the test set. We loop over all the batches in the test set and we record the value of the loss for each example separately. The examples with the highest reconstruction loss are our anomalies. \n",
    "\n",
    "Indeed, if the reconstruction loss is high, that means that our trained autoencoder could not reconstruct them well. Indeed, what the autoencoder learned about our dataset during training is not enough to describe these examples, which means they are different than what the encoder has seen during training, i.e., they are anomalies (or at least they are the most uncharacteristic examples).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this dataset is small we collect all the losses as well as\n",
    "# the image and its reconstruction in a dictionary. In case of a\n",
    "# larger dataset you might have to save on disk\n",
    "# (won't fit in memory)\n",
    "losses = {}\n",
    "\n",
    "# We need the loss by example (not by batch)\n",
    "loss_no_reduction = nn.MSELoss(reduction='none')\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(desc=\"Testing\", total=len(data_loaders['test']),\n",
    "            iterable=data_loaders['test']\n",
    "        ):\n",
    "\n",
    "            images, _ = data\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_no_reduction(outputs, images)\n",
    "            \n",
    "            # Accumulate results per-example\n",
    "            for i, l in enumerate(loss.mean(dim=[1, 2, 3])):\n",
    "                losses[idx + i] = {\n",
    "                    'loss': float(l.cpu().numpy()),\n",
    "                    'image': images[i].cpu().numpy(),\n",
    "                    'reconstructed': outputs[i].cpu().numpy()\n",
    "                }\n",
    "            \n",
    "            idx += loss.shape[0]\n",
    "\n",
    "# Let's save our results in a pandas DataFrame\n",
    "df = pd.DataFrame(losses).T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display the histogram of the loss. The elements on the right (with the higher loss) are the most uncharacteristic examples. Feel free to look into `helpers.py` to see how these plots are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import anomaly_detection_display\n",
    "\n",
    "anomaly_detection_display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got similar results as in the case of linear autoencoders, but the loss is on average much smaller (a sign that the network is much more capable of representing and reconstructing the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
